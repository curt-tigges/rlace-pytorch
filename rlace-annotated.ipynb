{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import time\n",
    "from torch.optim import SGD, Adam\n",
    "import random\n",
    "import sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-LACE Algorithm\n",
    "The code below reproduces the R-LACE algorithm as specified in the paper \":inear Adversarial Concept Erasure.\" A complete description of the algorithm is shown below.\n",
    "![rlace](images/r-lace-pseudocode.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_CLF_PARAMS = {\n",
    "    \"loss\": \"log_loss\",\n",
    "    \"tol\": 1e-4,\n",
    "    \"iters_no_change\": 15,\n",
    "    \"alpha\": 1e-4,\n",
    "    \"max_iter\": 25000,\n",
    "}\n",
    "NUM_CLFS_IN_EVAL = 3  # change to 1 for large dataset / high dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_classifier():\n",
    "\n",
    "    return SGDClassifier(\n",
    "        loss=EVAL_CLF_PARAMS[\"loss\"],\n",
    "        fit_intercept=True,\n",
    "        max_iter=EVAL_CLF_PARAMS[\"max_iter\"],\n",
    "        tol=EVAL_CLF_PARAMS[\"tol\"],\n",
    "        n_iter_no_change=EVAL_CLF_PARAMS[\"iters_no_change\"],\n",
    "        n_jobs=32,\n",
    "        alpha=EVAL_CLF_PARAMS[\"alpha\"],\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(X_train, y_train, X_dev, y_dev, P, rank):\n",
    "    \"\"\"Get the score of a projection matrix P on the dev set.\"\"\"\n",
    "    P_svd = get_projection(P, rank)\n",
    "\n",
    "    loss_vals = []\n",
    "    accs = []\n",
    "\n",
    "    for i in range(NUM_CLFS_IN_EVAL):\n",
    "        clf = init_classifier()\n",
    "        clf.fit(X_train @ P_svd, y_train)\n",
    "        y_pred = clf.predict_proba(X_dev @ P_svd)\n",
    "        loss = sklearn.metrics.log_loss(y_dev, y_pred)\n",
    "        loss_vals.append(loss)\n",
    "        accs.append(clf.score(X_dev @ P_svd, y_dev))\n",
    "\n",
    "    i = np.argmin(loss_vals)\n",
    "    return loss_vals[i], accs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_constraint(lambdas, d=1):\n",
    "    \"\"\"Solve the constraint sum(min(max(lambdas - theta, 0), 1)) = d for theta.\"\"\"\n",
    "\n",
    "    def f(theta):\n",
    "        return_val = np.sum(np.minimum(np.maximum(lambdas - theta, 0), 1)) - d\n",
    "        return return_val\n",
    "\n",
    "    theta_min, theta_max = max(lambdas), min(lambdas) - 1\n",
    "    assert f(theta_min) * f(theta_max) < 0\n",
    "\n",
    "    mid = (theta_min + theta_max) / 2\n",
    "    tol = 1e-4\n",
    "    iters = 0\n",
    "\n",
    "    while iters < 25:\n",
    "\n",
    "        mid = (theta_min + theta_max) / 2\n",
    "\n",
    "        if f(mid) * f(theta_min) > 0:\n",
    "\n",
    "            theta_min = mid\n",
    "        else:\n",
    "            theta_max = mid\n",
    "        iters += 1\n",
    "\n",
    "    lambdas_plus = np.minimum(np.maximum(lambdas - mid, 0), 1)\n",
    "    # if (theta_min-theta_max)**2 > tol:\n",
    "    #    print(\"didn't converge\", (theta_min-theta_max)**2)\n",
    "    return lambdas_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_acc(y):\n",
    "    \"\"\"Get the majority accuracy of a set of labels.\"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    c = Counter(y)\n",
    "    fracts = [v / sum(c.values()) for v in c.values()]\n",
    "    maj = max(fracts)\n",
    "    return maj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(y):\n",
    "    \"\"\"Get the entropy of a set of labels.\"\"\"\n",
    "\n",
    "    from collections import Counter\n",
    "    import scipy\n",
    "\n",
    "    c = Counter(y)\n",
    "    fracts = [v / sum(c.values()) for v in c.values()]\n",
    "    return scipy.stats.entropy(fracts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Algorithm Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes the projection matrix symmetric.\n",
    "![symmetric](images/symmetric.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric(X):\n",
    "    \"\"\"Make a matrix symmetric by averaging with its transpose.\"\"\"\n",
    "    X.data = 0.5 * (X.data + X.data.T)\n",
    "    return X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates an orthogonal projection matrix as specified below:\n",
    "![SVD for projection matrix](images/svd-projection-step.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projection(P, rank):\n",
    "    \"\"\"Get the projection matrix from the SVD of P.\"\"\"\n",
    "    D, U = np.linalg.eigh(P)\n",
    "    U = U.T\n",
    "    W = U[-rank:]\n",
    "    P_final = np.eye(P.shape[0]) - W.T @ W\n",
    "    return P_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints output before and after SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_output(P, rank, score):\n",
    "    \"\"\"Prepare the output of the algorithm.\"\"\"\n",
    "    P_final = get_projection(P, rank)\n",
    "    return {\"score\": score, \"P_before_svd\": np.eye(P.shape[0]) - P, \"P\": P_final}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below runs the inner and outer loops of the RLACE algorithm, as specified in the accompanying image.\n",
    "![Adversarial game](images/run-adversarial-game.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_adv_game(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_dev,\n",
    "    y_dev,\n",
    "    rank=1,\n",
    "    device=\"cpu\",\n",
    "    out_iters=75000,\n",
    "    in_iters_adv=1,\n",
    "    in_iters_clf=1,\n",
    "    epsilon=0.0015,\n",
    "    batch_size=128,\n",
    "    evalaute_every=1000,\n",
    "    optimizer_class=SGD,\n",
    "    optimizer_params_P={\"lr\": 0.005, \"weight_decay\": 1e-4},\n",
    "    optimizer_params_predictor={\"lr\": 0.005, \"weight_decay\": 1e-4},\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the adversarial game.\n",
    "\n",
    "    Args:\n",
    "        X_train: The training data\n",
    "        y_train: The training labels\n",
    "        X_dev: The dev data\n",
    "        y_dev: The dev labels\n",
    "        rank: The rank of the projection matrix\n",
    "        device: The device to run on\n",
    "        out_iters: The number of outer iterations\n",
    "        in_iters_adv: The number of inner iterations for the adversary\n",
    "        in_iters_clf: The number of inner iterations for the classifier\n",
    "        epsilon: The epsilon for the adversary\n",
    "        batch_size: The batch size\n",
    "        evalaute_every: Evaluate every n iterations\n",
    "        optimizer_class: The optimizer class\n",
    "        optimizer_params_P: The optimizer parameters for the projection matrix\n",
    "        optimizer_params_predictor: The optimizer parameters for the predictor\n",
    "\n",
    "    Returns:\n",
    "        The projection matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def get_loss_fn(X, y, predictor, P, bce_loss_fn, optimize_P=False):\n",
    "        \"\"\"Get the loss function.\n",
    "\n",
    "        Args:\n",
    "            X: The input\n",
    "            y: The labels\n",
    "            predictor: The predictor\n",
    "            P: The projection matrix\n",
    "            bce_loss_fn: The loss function\n",
    "            optimize_P: Whether to optimize the projection matrix or not. If True, the loss is negated.\n",
    "\n",
    "        Returns:\n",
    "            The loss\n",
    "        \"\"\"\n",
    "        I = torch.eye(X_train.shape[1]).to(device)\n",
    "        bce = bce_loss_fn(predictor(X @ (I - P)).squeeze(), y)\n",
    "        if optimize_P:\n",
    "            bce = -bce\n",
    "        return bce\n",
    "\n",
    "    X_torch = torch.tensor(X_train).float().to(device)\n",
    "    y_torch = torch.tensor(y_train).float().to(device)\n",
    "\n",
    "    num_labels = len(set(y_train.tolist()))\n",
    "\n",
    "    # set the loss function based on the number of labels\n",
    "    if num_labels == 2:\n",
    "        predictor = torch.nn.Linear(X_train.shape[1], 1).to(device)\n",
    "        bce_loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        y_torch = y_torch.float()\n",
    "    else:\n",
    "        predictor = torch.nn.Linear(X_train.shape[1], num_labels).to(device)\n",
    "        bce_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        y_torch = y_torch.long()\n",
    "\n",
    "    # initialize the projection matrix\n",
    "    P = 1e-1 * torch.randn(X_train.shape[1], X_train.shape[1]).to(device)\n",
    "    P.requires_grad = True\n",
    "\n",
    "    # initialize the optimizers\n",
    "    optimizer_predictor = optimizer_class(\n",
    "        predictor.parameters(), **optimizer_params_predictor\n",
    "    )\n",
    "    optimizer_P = optimizer_class([P], **optimizer_params_P)\n",
    "\n",
    "    # get the majority accuracy and entropy of the training set\n",
    "    maj = get_majority_acc(y_train)\n",
    "    label_entropy = get_entropy(y_train)\n",
    "    pbar = tqdm.tqdm(range(out_iters), total=out_iters, ascii=True, position=0, leave=True)\n",
    "    count_examples = 0\n",
    "    best_P, best_score, best_loss = None, 1, -1\n",
    "\n",
    "    # run the outer loop\n",
    "    for i in pbar:\n",
    "\n",
    "        for j in range(in_iters_adv):\n",
    "            # optimize the projection matrix\n",
    "            # this matches up with the second inner loop of the algorithm in the paper\n",
    "            P = symmetric(P)\n",
    "            optimizer_P.zero_grad()\n",
    "\n",
    "            idx = np.arange(0, X_torch.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            X_batch, y_batch = X_torch[idx[:batch_size]], y_torch[idx[:batch_size]]\n",
    "\n",
    "            loss_P = get_loss_fn(\n",
    "                X_batch, y_batch, predictor, symmetric(P), bce_loss_fn, optimize_P=True\n",
    "            )\n",
    "            loss_P.backward()\n",
    "            optimizer_P.step()\n",
    "\n",
    "            # project\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # project the projection matrix on the Fantope using lemma B.1\n",
    "                D, U = torch.linalg.eigh(symmetric(P).detach().cpu())\n",
    "                D = D.detach().cpu().numpy()\n",
    "                D_plus_diag = solve_constraint(D, d=rank)\n",
    "                D = torch.tensor(np.diag(D_plus_diag).real).float().to(device)\n",
    "                U = U.to(device)\n",
    "                P.data = U @ D @ U.T\n",
    "\n",
    "        for j in range(in_iters_clf):\n",
    "            # optimize the predictor\n",
    "            # this matches up with the first inner loop of the algorithm in the paper\n",
    "            optimizer_predictor.zero_grad()\n",
    "            idx = np.arange(0, X_torch.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            X_batch, y_batch = X_torch[idx[:batch_size]], y_torch[idx[:batch_size]]\n",
    "\n",
    "            loss_predictor = get_loss_fn(\n",
    "                X_batch, y_batch, predictor, symmetric(P), bce_loss_fn, optimize_P=False\n",
    "            )\n",
    "            loss_predictor.backward()\n",
    "            optimizer_predictor.step()\n",
    "            count_examples += batch_size\n",
    "\n",
    "        if i % evalaute_every == 0:\n",
    "            # pbar.set_description(\"Evaluating current adversary...\")\n",
    "            loss_val, score = get_score(\n",
    "                X_train, y_train, X_train, y_train, P.detach().cpu().numpy(), rank\n",
    "            )\n",
    "            if (\n",
    "                loss_val > best_loss\n",
    "            ):  # if np.abs(score - maj) < np.abs(best_score - maj):\n",
    "                best_P, best_loss = symmetric(P).detach().cpu().numpy().copy(), loss_val\n",
    "            if np.abs(score - maj) < np.abs(best_score - maj):\n",
    "                best_score = score\n",
    "\n",
    "            # update progress bar\n",
    "\n",
    "            best_so_far = (\n",
    "                best_score if np.abs(best_score - maj) < np.abs(score - maj) else score\n",
    "            )\n",
    "\n",
    "            pbar.set_description(\n",
    "                \"{:.0f}/{:.0f}. Acc post-projection: {:.3f}%; best so-far: {:.3f}%; Maj: {:.3f}%; Gap: {:.3f}%; best loss: {:.4f}; current loss: {:.4f}\".format(\n",
    "                    i,\n",
    "                    out_iters,\n",
    "                    score * 100,\n",
    "                    best_so_far * 100,\n",
    "                    maj * 100,\n",
    "                    np.abs(best_so_far - maj) * 100,\n",
    "                    best_loss,\n",
    "                    loss_val,\n",
    "                )\n",
    "            )\n",
    "            pbar.refresh()  # to show immediately the update\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        if i > 1 and np.abs(best_score - maj) < epsilon:\n",
    "            # if i > 1 and np.abs(best_loss - label_entropy) < epsilon:\n",
    "            break\n",
    "    output = prepare_output(best_P, rank, best_score)\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40000/50000. Acc post-projection: 50.200%; best so-far: 50.200%; Maj: 50.244%; Gap: 0.044%; best loss: 0.6932; current loss: 0.6932:  80%|########  | 40000/50000 [02:34<00:38, 259.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "Original Acc, dev: 96.717%; Acc, projected, no svd, dev: 49.033%; Acc, projected+SVD, train: 50.700%; Acc, projected+SVD, dev: 51.533%\n",
      "Majority Acc, dev: 50.983 %\n",
      "Majority Acc, train: 50.244 %\n",
      "Gap, dev: 0.550 %\n",
      "Gap, train: 0.456 %\n",
      "===================================================\n",
      "Eigenvalues, before SVD: [1.74330723e-07 9.99999992e-01 9.99999993e-01 9.99999995e-01\n",
      " 9.99999995e-01 9.99999995e-01 9.99999995e-01 9.99999996e-01\n",
      " 9.99999996e-01 9.99999996e-01 9.99999997e-01 9.99999997e-01\n",
      " 9.99999997e-01 9.99999997e-01 9.99999997e-01 9.99999997e-01\n",
      " 9.99999998e-01 9.99999998e-01 9.99999998e-01 9.99999998e-01\n",
      " 9.99999998e-01 9.99999998e-01 9.99999998e-01 9.99999998e-01\n",
      " 9.99999998e-01 9.99999998e-01 9.99999998e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000001e+00 1.00000001e+00 1.00000001e+00 1.00000001e+00]\n",
      "Eigenvalues, after SVD: [4.02957492e-09 9.99999993e-01 9.99999995e-01 9.99999995e-01\n",
      " 9.99999995e-01 9.99999995e-01 9.99999996e-01 9.99999996e-01\n",
      " 9.99999996e-01 9.99999996e-01 9.99999997e-01 9.99999997e-01\n",
      " 9.99999997e-01 9.99999997e-01 9.99999997e-01 9.99999998e-01\n",
      " 9.99999998e-01 9.99999998e-01 9.99999998e-01 9.99999998e-01\n",
      " 9.99999998e-01 9.99999998e-01 9.99999998e-01 9.99999998e-01\n",
      " 9.99999998e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000001e+00 1.00000001e+00 1.00000001e+00]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # random.seed(0)\n",
    "    # np.random.seed(0)\n",
    "\n",
    "    # create a synthetic dataset\n",
    "    n, dim = 15000, 200\n",
    "    num_classes = 2\n",
    "\n",
    "    X = np.random.randn(n, dim)\n",
    "    y = np.random.randint(\n",
    "        low=0, high=num_classes, size=n\n",
    "    )  # (np.random.rand(n) > 0.5).astype(int)\n",
    "\n",
    "    X[:, 0] = (y + np.random.randn(*y.shape) * 0.3) ** 2 + 0.3 * y\n",
    "    X[:, 1] = (y + np.random.randn(*y.shape) * 0.1) ** 2 - 0.7 * y\n",
    "    X[:, 2] = (\n",
    "        (y + np.random.randn(*y.shape) * 0.3) ** 2\n",
    "        + 0.5 * y\n",
    "        + np.random.randn(*y.shape) * 0.2\n",
    "    )\n",
    "    X[:, 3] = (\n",
    "        (y + np.random.randn(*y.shape) * 0.5) ** 2\n",
    "        - 0.7 * y\n",
    "        + np.random.randn(*y.shape) * 0.1\n",
    "    )\n",
    "    X[:, 4] = (\n",
    "        (y + np.random.randn(*y.shape) * 0.5) ** 2\n",
    "        - 0.8 * y\n",
    "        + np.random.randn(*y.shape) * 0.1\n",
    "    )\n",
    "    X[:, 5] = (\n",
    "        (y + np.random.randn(*y.shape) * 0.25) ** 2\n",
    "        - 0.2 * y\n",
    "        + np.random.randn(*y.shape) * 0.1\n",
    "    )\n",
    "    mixing_matrix = 1e-2 * np.random.randn(dim, dim)\n",
    "    X = X @ mixing_matrix\n",
    "\n",
    "    l_train = int(0.6 * n)\n",
    "    X_train, y_train = X[:l_train], y[:l_train]\n",
    "    X_dev, y_dev = X[l_train:], y[l_train:]\n",
    "\n",
    "    # arguments\n",
    "    num_iters = 50000\n",
    "    rank = 1\n",
    "    optimizer_class = torch.optim.SGD\n",
    "    optimizer_params_P = {\"lr\": 0.003, \"weight_decay\": 1e-4}\n",
    "    optimizer_params_predictor = {\"lr\": 0.003, \"weight_decay\": 1e-4}\n",
    "    epsilon = 0.001  # stop 0.1% from majority acc\n",
    "    batch_size = 256\n",
    "\n",
    "    output = solve_adv_game(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_dev,\n",
    "        y_dev,\n",
    "        rank=rank,\n",
    "        device=\"cpu\",\n",
    "        out_iters=num_iters,\n",
    "        optimizer_class=optimizer_class,\n",
    "        optimizer_params_P=optimizer_params_P,\n",
    "        optimizer_params_predictor=optimizer_params_predictor,\n",
    "        epsilon=epsilon,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # train a classifier\n",
    "\n",
    "    P_svd = output[\"P\"]\n",
    "    P_before_svd = output[\"P_before_svd\"]\n",
    "    svm = init_classifier()\n",
    "\n",
    "    svm.fit(X_train[:], y_train[:])\n",
    "    score_original = svm.score(X_dev, y_dev)\n",
    "\n",
    "    svm = init_classifier()\n",
    "    svm.fit(X_train[:] @ P_before_svd, y_train[:])\n",
    "    score_projected_no_svd = svm.score(X_dev @ P_before_svd, y_dev)\n",
    "\n",
    "    svm = init_classifier()\n",
    "    svm.fit(X_train[:] @ P_svd, y_train[:])\n",
    "    score_projected_svd_dev = svm.score(X_dev @ P_svd, y_dev)\n",
    "    score_projected_svd_train = svm.score(X_train @ P_svd, y_train)\n",
    "    maj_acc_dev = get_majority_acc(y_dev)\n",
    "    maj_acc_train = get_majority_acc(y_train)\n",
    "\n",
    "    print(\"===================================================\")\n",
    "    print(\n",
    "        \"Original Acc, dev: {:.3f}%; Acc, projected, no svd, dev: {:.3f}%; Acc, projected+SVD, train: {:.3f}%; Acc, projected+SVD, dev: {:.3f}%\".format(\n",
    "            score_original * 100,\n",
    "            score_projected_no_svd * 100,\n",
    "            score_projected_svd_train * 100,\n",
    "            score_projected_svd_dev * 100,\n",
    "        )\n",
    "    )\n",
    "    print(\"Majority Acc, dev: {:.3f} %\".format(maj_acc_dev * 100))\n",
    "    print(\"Majority Acc, train: {:.3f} %\".format(maj_acc_train * 100))\n",
    "    print(\n",
    "        \"Gap, dev: {:.3f} %\".format(np.abs(maj_acc_dev - score_projected_svd_dev) * 100)\n",
    "    )\n",
    "    print(\n",
    "        \"Gap, train: {:.3f} %\".format(\n",
    "            np.abs(maj_acc_train - score_projected_svd_train) * 100\n",
    "        )\n",
    "    )\n",
    "    print(\"===================================================\")\n",
    "    eigs_before_svd, _ = np.linalg.eigh(P_before_svd)\n",
    "    print(\"Eigenvalues, before SVD: {}\".format(eigs_before_svd[:]))\n",
    "\n",
    "    eigs_after_svd, _ = np.linalg.eigh(P_svd)\n",
    "    print(\"Eigenvalues, after SVD: {}\".format(eigs_after_svd[:]))\n",
    "\n",
    "    eps = 1e-6\n",
    "    assert np.abs((eigs_after_svd > eps).sum() - (dim - rank)) < eps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0dda8425f5b2686a7d4649dd29b2ea64fce78f14efba812f7700f06d7544e589"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
