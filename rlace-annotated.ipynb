{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import time\n",
    "from torch.optim import SGD, Adam\n",
    "import random\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_CLF_PARAMS = {\n",
    "    \"loss\": \"log\",\n",
    "    \"tol\": 1e-4,\n",
    "    \"iters_no_change\": 15,\n",
    "    \"alpha\": 1e-4,\n",
    "    \"max_iter\": 25000,\n",
    "}\n",
    "NUM_CLFS_IN_EVAL = 3  # change to 1 for large dataset / high dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_classifier():\n",
    "\n",
    "    return SGDClassifier(\n",
    "        loss=EVAL_CLF_PARAMS[\"loss\"],\n",
    "        fit_intercept=True,\n",
    "        max_iter=EVAL_CLF_PARAMS[\"max_iter\"],\n",
    "        tol=EVAL_CLF_PARAMS[\"tol\"],\n",
    "        n_iter_no_change=EVAL_CLF_PARAMS[\"iters_no_change\"],\n",
    "        n_jobs=32,\n",
    "        alpha=EVAL_CLF_PARAMS[\"alpha\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric(X):\n",
    "    \"\"\"Make a matrix symmetric by averaging with its transpose.\"\"\"\n",
    "    X.data = 0.5 * (X.data + X.data.T)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(X_train, y_train, X_dev, y_dev, P, rank):\n",
    "    \"\"\"Get the score of a projection matrix P on the dev set.\"\"\"\n",
    "    P_svd = get_projection(P, rank)\n",
    "\n",
    "    loss_vals = []\n",
    "    accs = []\n",
    "\n",
    "    for i in range(NUM_CLFS_IN_EVAL):\n",
    "        clf = init_classifier()\n",
    "        clf.fit(X_train @ P_svd, y_train)\n",
    "        y_pred = clf.predict_proba(X_dev @ P_svd)\n",
    "        loss = sklearn.metrics.log_loss(y_dev, y_pred)\n",
    "        loss_vals.append(loss)\n",
    "        accs.append(clf.score(X_dev @ P_svd, y_dev))\n",
    "\n",
    "    i = np.argmin(loss_vals)\n",
    "    return loss_vals[i], accs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_constraint(lambdas, d=1):\n",
    "    \"\"\"Solve the constraint sum(min(max(lambdas - theta, 0), 1)) = d for theta.\"\"\"\n",
    "\n",
    "    def f(theta):\n",
    "        return_val = np.sum(np.minimum(np.maximum(lambdas - theta, 0), 1)) - d\n",
    "        return return_val\n",
    "\n",
    "    theta_min, theta_max = max(lambdas), min(lambdas) - 1\n",
    "    assert f(theta_min) * f(theta_max) < 0\n",
    "\n",
    "    mid = (theta_min + theta_max) / 2\n",
    "    tol = 1e-4\n",
    "    iters = 0\n",
    "\n",
    "    while iters < 25:\n",
    "\n",
    "        mid = (theta_min + theta_max) / 2\n",
    "\n",
    "        if f(mid) * f(theta_min) > 0:\n",
    "\n",
    "            theta_min = mid\n",
    "        else:\n",
    "            theta_max = mid\n",
    "        iters += 1\n",
    "\n",
    "    lambdas_plus = np.minimum(np.maximum(lambdas - mid, 0), 1)\n",
    "    # if (theta_min-theta_max)**2 > tol:\n",
    "    #    print(\"didn't converge\", (theta_min-theta_max)**2)\n",
    "    return lambdas_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_acc(y):\n",
    "    \"\"\"Get the majority accuracy of a set of labels.\"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    c = Counter(y)\n",
    "    fracts = [v / sum(c.values()) for v in c.values()]\n",
    "    maj = max(fracts)\n",
    "    return maj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(y):\n",
    "    \"\"\"Get the entropy of a set of labels.\"\"\"\n",
    "\n",
    "    from collections import Counter\n",
    "    import scipy\n",
    "\n",
    "    c = Counter(y)\n",
    "    fracts = [v / sum(c.values()) for v in c.values()]\n",
    "    return scipy.stats.entropy(fracts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes the projection matrix symmetric.\n",
    "![symmetric](images/symmetric.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric(X):\n",
    "    \"\"\"Make a matrix symmetric by averaging with its transpose.\"\"\"\n",
    "    X.data = 0.5 * (X.data + X.data.T)\n",
    "    return X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates an orthogonal projection matrix as specified below:\n",
    "![SVD for projection matrix](images/svd-projection-step.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projection(P, rank):\n",
    "    \"\"\"Get the projection matrix from the SVD of P.\"\"\"\n",
    "    D, U = np.linalg.eigh(P)\n",
    "    U = U.T\n",
    "    W = U[-rank:]\n",
    "    P_final = np.eye(P.shape[0]) - W.T @ W\n",
    "    return P_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints output before and after SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_output(P, rank, score):\n",
    "    \"\"\"Prepare the output of the algorithm.\"\"\"\n",
    "    P_final = get_projection(P, rank)\n",
    "    return {\"score\": score, \"P_before_svd\": np.eye(P.shape[0]) - P, \"P\": P_final}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below runs the inner and outer loops of the RLACE algorithm, as specified in the accompanying image.\n",
    "![Adversarial game](images/run-adversarial-game.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_adv_game(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_dev,\n",
    "    y_dev,\n",
    "    rank=1,\n",
    "    device=\"cpu\",\n",
    "    out_iters=75000,\n",
    "    in_iters_adv=1,\n",
    "    in_iters_clf=1,\n",
    "    epsilon=0.0015,\n",
    "    batch_size=128,\n",
    "    evalaute_every=1000,\n",
    "    optimizer_class=SGD,\n",
    "    optimizer_params_P={\"lr\": 0.005, \"weight_decay\": 1e-4},\n",
    "    optimizer_params_predictor={\"lr\": 0.005, \"weight_decay\": 1e-4},\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the adversarial game.\n",
    "\n",
    "    Args:\n",
    "        X_train: The training data\n",
    "        y_train: The training labels\n",
    "        X_dev: The dev data\n",
    "        y_dev: The dev labels\n",
    "        rank: The rank of the projection matrix\n",
    "        device: The device to run on\n",
    "        out_iters: The number of outer iterations\n",
    "        in_iters_adv: The number of inner iterations for the adversary\n",
    "        in_iters_clf: The number of inner iterations for the classifier\n",
    "        epsilon: The epsilon for the adversary\n",
    "        batch_size: The batch size\n",
    "        evalaute_every: Evaluate every n iterations\n",
    "        optimizer_class: The optimizer class\n",
    "        optimizer_params_P: The optimizer parameters for the projection matrix\n",
    "        optimizer_params_predictor: The optimizer parameters for the predictor\n",
    "\n",
    "    Returns:\n",
    "        The projection matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def get_loss_fn(X, y, predictor, P, bce_loss_fn, optimize_P=False):\n",
    "        \"\"\"Get the loss function.\n",
    "\n",
    "        Args:\n",
    "            X: The input\n",
    "            y: The labels\n",
    "            predictor: The predictor\n",
    "            P: The projection matrix\n",
    "            bce_loss_fn: The loss function\n",
    "            optimize_P: Whether to optimize the projection matrix or not. If True, the loss is negated.\n",
    "\n",
    "        Returns:\n",
    "            The loss\n",
    "        \"\"\"\n",
    "        I = torch.eye(X_train.shape[1]).to(device)\n",
    "        bce = bce_loss_fn(predictor(X @ (I - P)).squeeze(), y)\n",
    "        if optimize_P:\n",
    "            bce = -bce\n",
    "        return bce\n",
    "\n",
    "    X_torch = torch.tensor(X_train).float().to(device)\n",
    "    y_torch = torch.tensor(y_train).float().to(device)\n",
    "\n",
    "    num_labels = len(set(y_train.tolist()))\n",
    "\n",
    "    # set the loss function based on the number of labels\n",
    "    if num_labels == 2:\n",
    "        predictor = torch.nn.Linear(X_train.shape[1], 1).to(device)\n",
    "        bce_loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        y_torch = y_torch.float()\n",
    "    else:\n",
    "        predictor = torch.nn.Linear(X_train.shape[1], num_labels).to(device)\n",
    "        bce_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        y_torch = y_torch.long()\n",
    "\n",
    "    # initialize the projection matrix\n",
    "    P = 1e-1 * torch.randn(X_train.shape[1], X_train.shape[1]).to(device)\n",
    "    P.requires_grad = True\n",
    "\n",
    "    # initialize the optimizers\n",
    "    optimizer_predictor = optimizer_class(\n",
    "        predictor.parameters(), **optimizer_params_predictor\n",
    "    )\n",
    "    optimizer_P = optimizer_class([P], **optimizer_params_P)\n",
    "\n",
    "    # get the majority accuracy and entropy of the training set\n",
    "    maj = get_majority_acc(y_train)\n",
    "    label_entropy = get_entropy(y_train)\n",
    "    pbar = tqdm.tqdm(range(out_iters), total=out_iters, ascii=True)\n",
    "    count_examples = 0\n",
    "    best_P, best_score, best_loss = None, 1, -1\n",
    "\n",
    "    # run the outer loop\n",
    "    for i in pbar:\n",
    "\n",
    "        for j in range(in_iters_adv):\n",
    "            # optimize the projection matrix\n",
    "            # this matches up with the second inner loop of the algorithm in the paper\n",
    "            P = symmetric(P)\n",
    "            optimizer_P.zero_grad()\n",
    "\n",
    "            idx = np.arange(0, X_torch.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            X_batch, y_batch = X_torch[idx[:batch_size]], y_torch[idx[:batch_size]]\n",
    "\n",
    "            loss_P = get_loss_fn(\n",
    "                X_batch, y_batch, predictor, symmetric(P), bce_loss_fn, optimize_P=True\n",
    "            )\n",
    "            loss_P.backward()\n",
    "            optimizer_P.step()\n",
    "\n",
    "            # project\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # project the projection matrix on the Fantope using lemma B.1\n",
    "                D, U = torch.linalg.eigh(symmetric(P).detach().cpu())\n",
    "                D = D.detach().cpu().numpy()\n",
    "                D_plus_diag = solve_constraint(D, d=rank)\n",
    "                D = torch.tensor(np.diag(D_plus_diag).real).float().to(device)\n",
    "                U = U.to(device)\n",
    "                P.data = U @ D @ U.T\n",
    "\n",
    "        for j in range(in_iters_clf):\n",
    "            # optimize the predictor\n",
    "            # this matches up with the first inner loop of the algorithm in the paper\n",
    "            optimizer_predictor.zero_grad()\n",
    "            idx = np.arange(0, X_torch.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            X_batch, y_batch = X_torch[idx[:batch_size]], y_torch[idx[:batch_size]]\n",
    "\n",
    "            loss_predictor = get_loss_fn(\n",
    "                X_batch, y_batch, predictor, symmetric(P), bce_loss_fn, optimize_P=False\n",
    "            )\n",
    "            loss_predictor.backward()\n",
    "            optimizer_predictor.step()\n",
    "            count_examples += batch_size\n",
    "\n",
    "        if i % evalaute_every == 0:\n",
    "            # pbar.set_description(\"Evaluating current adversary...\")\n",
    "            loss_val, score = get_score(\n",
    "                X_train, y_train, X_train, y_train, P.detach().cpu().numpy(), rank\n",
    "            )\n",
    "            if (\n",
    "                loss_val > best_loss\n",
    "            ):  # if np.abs(score - maj) < np.abs(best_score - maj):\n",
    "                best_P, best_loss = symmetric(P).detach().cpu().numpy().copy(), loss_val\n",
    "            if np.abs(score - maj) < np.abs(best_score - maj):\n",
    "                best_score = score\n",
    "\n",
    "            # update progress bar\n",
    "\n",
    "            best_so_far = (\n",
    "                best_score if np.abs(best_score - maj) < np.abs(score - maj) else score\n",
    "            )\n",
    "\n",
    "            pbar.set_description(\n",
    "                \"{:.0f}/{:.0f}. Acc post-projection: {:.3f}%; best so-far: {:.3f}%; Maj: {:.3f}%; Gap: {:.3f}%; best loss: {:.4f}; current loss: {:.4f}\".format(\n",
    "                    i,\n",
    "                    out_iters,\n",
    "                    score * 100,\n",
    "                    best_so_far * 100,\n",
    "                    maj * 100,\n",
    "                    np.abs(best_so_far - maj) * 100,\n",
    "                    best_loss,\n",
    "                    loss_val,\n",
    "                )\n",
    "            )\n",
    "            pbar.refresh()  # to show immediately the update\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        if i > 1 and np.abs(best_score - maj) < epsilon:\n",
    "            # if i > 1 and np.abs(best_loss - label_entropy) < epsilon:\n",
    "            break\n",
    "    output = prepare_output(best_P, rank, best_score)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "0/50000. Acc post-projection: 96.633%; best so-far: 96.633%; Maj: 50.822%; Gap: 45.811%; best loss: 0.1272; current loss: 0.1272:   2%|1         | 983/50000 [00:03<02:10, 374.53it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "1000/50000. Acc post-projection: 96.689%; best so-far: 96.633%; Maj: 50.822%; Gap: 45.811%; best loss: 0.1273; current loss: 0.1273:   4%|3         | 1971/50000 [00:06<02:08, 375.18it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "2000/50000. Acc post-projection: 96.711%; best so-far: 96.633%; Maj: 50.822%; Gap: 45.811%; best loss: 0.1279; current loss: 0.1279:   6%|5         | 2994/50000 [00:10<02:06, 371.77it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "3000/50000. Acc post-projection: 92.356%; best so-far: 92.356%; Maj: 50.822%; Gap: 41.533%; best loss: 0.2577; current loss: 0.2577:   8%|7         | 3982/50000 [00:13<02:03, 372.15it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "4000/50000. Acc post-projection: 89.622%; best so-far: 89.622%; Maj: 50.822%; Gap: 38.800%; best loss: 0.3221; current loss: 0.3221:  10%|9         | 4965/50000 [00:16<02:00, 373.97it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "5000/50000. Acc post-projection: 87.856%; best so-far: 87.856%; Maj: 50.822%; Gap: 37.033%; best loss: 0.3599; current loss: 0.3599:  12%|#1        | 5988/50000 [00:20<01:57, 374.91it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "6000/50000. Acc post-projection: 85.811%; best so-far: 85.811%; Maj: 50.822%; Gap: 34.989%; best loss: 0.3934; current loss: 0.3934:  14%|#3        | 6971/50000 [00:23<01:58, 363.81it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "7000/50000. Acc post-projection: 84.122%; best so-far: 84.122%; Maj: 50.822%; Gap: 33.300%; best loss: 0.4241; current loss: 0.4241:  16%|#5        | 7991/50000 [00:27<01:54, 367.81it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "8000/50000. Acc post-projection: 82.400%; best so-far: 82.400%; Maj: 50.822%; Gap: 31.578%; best loss: 0.4530; current loss: 0.4530:  18%|#7        | 8974/50000 [00:31<01:52, 364.34it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "9000/50000. Acc post-projection: 80.356%; best so-far: 80.356%; Maj: 50.822%; Gap: 29.533%; best loss: 0.4815; current loss: 0.4815:  20%|#9        | 9988/50000 [00:35<01:49, 366.03it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "10000/50000. Acc post-projection: 78.000%; best so-far: 78.000%; Maj: 50.822%; Gap: 27.178%; best loss: 0.5106; current loss: 0.5106:  22%|##1       | 10973/50000 [00:38<01:44, 373.93it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "11000/50000. Acc post-projection: 75.633%; best so-far: 75.633%; Maj: 50.822%; Gap: 24.811%; best loss: 0.5394; current loss: 0.5394:  24%|##3       | 11993/50000 [00:43<01:43, 368.65it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "12000/50000. Acc post-projection: 73.011%; best so-far: 73.011%; Maj: 50.822%; Gap: 22.189%; best loss: 0.5671; current loss: 0.5671:  26%|##5       | 12970/50000 [00:46<01:40, 368.23it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "13000/50000. Acc post-projection: 70.211%; best so-far: 70.211%; Maj: 50.822%; Gap: 19.389%; best loss: 0.5927; current loss: 0.5927:  28%|##7       | 13985/50000 [00:50<01:37, 370.75it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "14000/50000. Acc post-projection: 67.889%; best so-far: 67.889%; Maj: 50.822%; Gap: 17.067%; best loss: 0.6156; current loss: 0.6156:  30%|##9       | 14972/50000 [00:54<01:33, 376.25it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "15000/50000. Acc post-projection: 65.211%; best so-far: 65.211%; Maj: 50.822%; Gap: 14.389%; best loss: 0.6350; current loss: 0.6350:  32%|###1      | 15986/50000 [00:58<01:32, 367.29it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "16000/50000. Acc post-projection: 63.156%; best so-far: 63.156%; Maj: 50.822%; Gap: 12.333%; best loss: 0.6511; current loss: 0.6511:  34%|###3      | 16971/50000 [01:02<01:29, 370.58it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "17000/50000. Acc post-projection: 60.833%; best so-far: 60.833%; Maj: 50.822%; Gap: 10.011%; best loss: 0.6637; current loss: 0.6637:  36%|###5      | 17991/50000 [01:06<01:26, 371.87it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "18000/50000. Acc post-projection: 58.922%; best so-far: 58.922%; Maj: 50.822%; Gap: 8.100%; best loss: 0.6733; current loss: 0.6733:  38%|###7      | 18974/50000 [01:10<01:23, 371.71it/s] /home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "19000/50000. Acc post-projection: 57.156%; best so-far: 57.156%; Maj: 50.822%; Gap: 6.333%; best loss: 0.6801; current loss: 0.6801:  40%|###9      | 19980/50000 [01:14<01:28, 338.38it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "20000/50000. Acc post-projection: 55.178%; best so-far: 55.178%; Maj: 50.822%; Gap: 4.356%; best loss: 0.6850; current loss: 0.6850:  42%|####1     | 20987/50000 [01:17<01:18, 370.90it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "21000/50000. Acc post-projection: 54.567%; best so-far: 54.567%; Maj: 50.822%; Gap: 3.744%; best loss: 0.6883; current loss: 0.6883:  44%|####3     | 21966/50000 [01:21<01:16, 365.49it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "22000/50000. Acc post-projection: 54.056%; best so-far: 54.056%; Maj: 50.822%; Gap: 3.233%; best loss: 0.6905; current loss: 0.6905:  46%|####5     | 22982/50000 [01:25<01:13, 367.54it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "23000/50000. Acc post-projection: 52.422%; best so-far: 52.422%; Maj: 50.822%; Gap: 1.600%; best loss: 0.6916; current loss: 0.6916:  48%|####7     | 23998/50000 [01:29<01:12, 360.07it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "24000/50000. Acc post-projection: 52.344%; best so-far: 52.344%; Maj: 50.822%; Gap: 1.522%; best loss: 0.6922; current loss: 0.6922:  50%|####9     | 24982/50000 [01:33<01:07, 371.22it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "25000/50000. Acc post-projection: 51.744%; best so-far: 51.744%; Maj: 50.822%; Gap: 0.922%; best loss: 0.6925; current loss: 0.6925:  52%|#####1    | 25967/50000 [01:37<01:04, 375.36it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "26000/50000. Acc post-projection: 51.189%; best so-far: 51.189%; Maj: 50.822%; Gap: 0.367%; best loss: 0.6926; current loss: 0.6926:  54%|#####3    | 26989/50000 [01:42<01:01, 372.67it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "27000/50000. Acc post-projection: 51.000%; best so-far: 51.000%; Maj: 50.822%; Gap: 0.178%; best loss: 0.6926; current loss: 0.6926:  56%|#####5    | 27971/50000 [01:45<01:00, 364.58it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "28000/50000. Acc post-projection: 51.356%; best so-far: 51.000%; Maj: 50.822%; Gap: 0.178%; best loss: 0.6926; current loss: 0.6926:  58%|#####7    | 28992/50000 [01:50<00:56, 371.16it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "29000/50000. Acc post-projection: 51.400%; best so-far: 51.000%; Maj: 50.822%; Gap: 0.178%; best loss: 0.6926; current loss: 0.6926:  60%|#####9    | 29980/50000 [01:53<00:53, 371.91it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "30000/50000. Acc post-projection: 51.622%; best so-far: 51.000%; Maj: 50.822%; Gap: 0.178%; best loss: 0.6926; current loss: 0.6925:  62%|######1   | 30994/50000 [01:58<00:52, 360.77it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "31000/50000. Acc post-projection: 51.844%; best so-far: 51.000%; Maj: 50.822%; Gap: 0.178%; best loss: 0.6926; current loss: 0.6925:  64%|######3   | 31989/50000 [02:02<00:48, 372.63it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "32000/50000. Acc post-projection: 51.578%; best so-far: 51.000%; Maj: 50.822%; Gap: 0.178%; best loss: 0.6926; current loss: 0.6925:  66%|######5   | 32971/50000 [02:06<00:46, 369.50it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "33000/50000. Acc post-projection: 51.589%; best so-far: 51.000%; Maj: 50.822%; Gap: 0.178%; best loss: 0.6926; current loss: 0.6926:  68%|######7   | 33972/50000 [02:10<00:43, 367.38it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "34000/50000. Acc post-projection: 50.489%; best so-far: 51.000%; Maj: 50.822%; Gap: 0.178%; best loss: 0.6928; current loss: 0.6928:  70%|######9   | 34985/50000 [02:14<00:42, 357.05it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "35000/50000. Acc post-projection: 51.644%; best so-far: 51.000%; Maj: 50.822%; Gap: 0.178%; best loss: 0.6928; current loss: 0.6926:  72%|#######1  | 35968/50000 [02:18<00:37, 374.36it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "36000/50000. Acc post-projection: 51.100%; best so-far: 51.000%; Maj: 50.822%; Gap: 0.178%; best loss: 0.6928; current loss: 0.6926:  74%|#######3  | 36993/50000 [02:22<00:34, 375.62it/s]/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "37000/50000. Acc post-projection: 50.856%; best so-far: 50.856%; Maj: 50.822%; Gap: 0.033%; best loss: 0.6931; current loss: 0.6931:  74%|#######4  | 37000/50000 [02:24<00:50, 256.87it/s]\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "/home/curttigges/miniconda3/envs/rlace/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "Original Acc, dev: 96.567%; Acc, projected, no svd, dev: 49.917%; Acc, projected+SVD, train: 50.833%; Acc, projected+SVD, dev: 49.950%\n",
      "Majority Acc, dev: 50.017 %\n",
      "Majority Acc, train: 50.822 %\n",
      "Gap, dev: 0.067 %\n",
      "Gap, train: 0.011 %\n",
      "===================================================\n",
      "Eigenvalues, before SVD: [6.79300139e-07 9.99999993e-01 9.99999994e-01 9.99999995e-01\n",
      " 9.99999995e-01 9.99999995e-01 9.99999996e-01 9.99999996e-01\n",
      " 9.99999996e-01 9.99999996e-01 9.99999996e-01 9.99999997e-01\n",
      " 9.99999997e-01 9.99999997e-01 9.99999997e-01 9.99999997e-01\n",
      " 9.99999997e-01 9.99999998e-01 9.99999998e-01 9.99999998e-01\n",
      " 9.99999998e-01 9.99999998e-01 9.99999998e-01 9.99999998e-01\n",
      " 9.99999998e-01 9.99999998e-01 9.99999998e-01 9.99999998e-01\n",
      " 9.99999998e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000001e+00 1.00000001e+00 1.00000001e+00 1.00000001e+00]\n",
      "Eigenvalues, after SVD: [1.61480068e-09 9.99999993e-01 9.99999994e-01 9.99999995e-01\n",
      " 9.99999995e-01 9.99999996e-01 9.99999996e-01 9.99999996e-01\n",
      " 9.99999996e-01 9.99999997e-01 9.99999997e-01 9.99999997e-01\n",
      " 9.99999997e-01 9.99999997e-01 9.99999998e-01 9.99999998e-01\n",
      " 9.99999998e-01 9.99999998e-01 9.99999998e-01 9.99999998e-01\n",
      " 9.99999998e-01 9.99999998e-01 9.99999998e-01 9.99999998e-01\n",
      " 9.99999998e-01 9.99999998e-01 9.99999998e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 9.99999999e-01 9.99999999e-01 9.99999999e-01\n",
      " 9.99999999e-01 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000001e+00 1.00000001e+00 1.00000001e+00]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # random.seed(0)\n",
    "    # np.random.seed(0)\n",
    "\n",
    "    # create a synthetic dataset\n",
    "    n, dim = 15000, 200\n",
    "    num_classes = 2\n",
    "\n",
    "    X = np.random.randn(n, dim)\n",
    "    y = np.random.randint(\n",
    "        low=0, high=num_classes, size=n\n",
    "    )  # (np.random.rand(n) > 0.5).astype(int)\n",
    "\n",
    "    X[:, 0] = (y + np.random.randn(*y.shape) * 0.3) ** 2 + 0.3 * y\n",
    "    X[:, 1] = (y + np.random.randn(*y.shape) * 0.1) ** 2 - 0.7 * y\n",
    "    X[:, 2] = (\n",
    "        (y + np.random.randn(*y.shape) * 0.3) ** 2\n",
    "        + 0.5 * y\n",
    "        + np.random.randn(*y.shape) * 0.2\n",
    "    )\n",
    "    X[:, 3] = (\n",
    "        (y + np.random.randn(*y.shape) * 0.5) ** 2\n",
    "        - 0.7 * y\n",
    "        + np.random.randn(*y.shape) * 0.1\n",
    "    )\n",
    "    X[:, 4] = (\n",
    "        (y + np.random.randn(*y.shape) * 0.5) ** 2\n",
    "        - 0.8 * y\n",
    "        + np.random.randn(*y.shape) * 0.1\n",
    "    )\n",
    "    X[:, 5] = (\n",
    "        (y + np.random.randn(*y.shape) * 0.25) ** 2\n",
    "        - 0.2 * y\n",
    "        + np.random.randn(*y.shape) * 0.1\n",
    "    )\n",
    "    mixing_matrix = 1e-2 * np.random.randn(dim, dim)\n",
    "    X = X @ mixing_matrix\n",
    "\n",
    "    l_train = int(0.6 * n)\n",
    "    X_train, y_train = X[:l_train], y[:l_train]\n",
    "    X_dev, y_dev = X[l_train:], y[l_train:]\n",
    "\n",
    "    # arguments\n",
    "    num_iters = 50000\n",
    "    rank = 1\n",
    "    optimizer_class = torch.optim.SGD\n",
    "    optimizer_params_P = {\"lr\": 0.003, \"weight_decay\": 1e-4}\n",
    "    optimizer_params_predictor = {\"lr\": 0.003, \"weight_decay\": 1e-4}\n",
    "    epsilon = 0.001  # stop 0.1% from majority acc\n",
    "    batch_size = 256\n",
    "\n",
    "    output = solve_adv_game(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_dev,\n",
    "        y_dev,\n",
    "        rank=rank,\n",
    "        device=\"cpu\",\n",
    "        out_iters=num_iters,\n",
    "        optimizer_class=optimizer_class,\n",
    "        optimizer_params_P=optimizer_params_P,\n",
    "        optimizer_params_predictor=optimizer_params_predictor,\n",
    "        epsilon=epsilon,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # train a classifier\n",
    "\n",
    "    P_svd = output[\"P\"]\n",
    "    P_before_svd = output[\"P_before_svd\"]\n",
    "    svm = init_classifier()\n",
    "\n",
    "    svm.fit(X_train[:], y_train[:])\n",
    "    score_original = svm.score(X_dev, y_dev)\n",
    "\n",
    "    svm = init_classifier()\n",
    "    svm.fit(X_train[:] @ P_before_svd, y_train[:])\n",
    "    score_projected_no_svd = svm.score(X_dev @ P_before_svd, y_dev)\n",
    "\n",
    "    svm = init_classifier()\n",
    "    svm.fit(X_train[:] @ P_svd, y_train[:])\n",
    "    score_projected_svd_dev = svm.score(X_dev @ P_svd, y_dev)\n",
    "    score_projected_svd_train = svm.score(X_train @ P_svd, y_train)\n",
    "    maj_acc_dev = get_majority_acc(y_dev)\n",
    "    maj_acc_train = get_majority_acc(y_train)\n",
    "\n",
    "    print(\"===================================================\")\n",
    "    print(\n",
    "        \"Original Acc, dev: {:.3f}%; Acc, projected, no svd, dev: {:.3f}%; Acc, projected+SVD, train: {:.3f}%; Acc, projected+SVD, dev: {:.3f}%\".format(\n",
    "            score_original * 100,\n",
    "            score_projected_no_svd * 100,\n",
    "            score_projected_svd_train * 100,\n",
    "            score_projected_svd_dev * 100,\n",
    "        )\n",
    "    )\n",
    "    print(\"Majority Acc, dev: {:.3f} %\".format(maj_acc_dev * 100))\n",
    "    print(\"Majority Acc, train: {:.3f} %\".format(maj_acc_train * 100))\n",
    "    print(\n",
    "        \"Gap, dev: {:.3f} %\".format(np.abs(maj_acc_dev - score_projected_svd_dev) * 100)\n",
    "    )\n",
    "    print(\n",
    "        \"Gap, train: {:.3f} %\".format(\n",
    "            np.abs(maj_acc_train - score_projected_svd_train) * 100\n",
    "        )\n",
    "    )\n",
    "    print(\"===================================================\")\n",
    "    eigs_before_svd, _ = np.linalg.eigh(P_before_svd)\n",
    "    print(\"Eigenvalues, before SVD: {}\".format(eigs_before_svd[:]))\n",
    "\n",
    "    eigs_after_svd, _ = np.linalg.eigh(P_svd)\n",
    "    print(\"Eigenvalues, after SVD: {}\".format(eigs_after_svd[:]))\n",
    "\n",
    "    eps = 1e-6\n",
    "    assert np.abs((eigs_after_svd > eps).sum() - (dim - rank)) < eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0dda8425f5b2686a7d4649dd29b2ea64fce78f14efba812f7700f06d7544e589"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
