{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "\n",
    "import torch\n",
    "import circuitsvis as cv\n",
    "from transformers import AutoModelForCausalLM, GPT2ForSequenceClassification\n",
    "import transformer_lens as tl\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from torchtyping import TensorType as TT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-LACE Basic GPT-2 Experiment Procedure - Simple sentiment version\n",
    "1. Load HF classifier\n",
    "2. Identify target region\n",
    "3. Get activations in target region\n",
    "4. Load sentiment training data\n",
    "5. Perform R-LACE on these activations\n",
    "\n",
    "6. Replace old activations?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "set_seed(123)\n",
    "\n",
    "# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).\n",
    "epochs = 4\n",
    "\n",
    "# Number of batches - depending on the max sequence length and GPU memory.\n",
    "# For 512 sequence length batch of 10 works without cuda memory issues.\n",
    "# For small sequence length can try batch of 32 or higher.\n",
    "batch_size = 128\n",
    "\n",
    "# Pad or truncate text sequences to a specific length\n",
    "# if `None` it will use maximum sequence of word piece tokens allowed by model.\n",
    "max_length = 60\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Name of transformers model - will use already pretrained model.\n",
    "# Path of transformer model - will load your own model from local disk.\n",
    "model_name_or_path = 'gpt2'\n",
    "\n",
    "# Dictionary of labels and their id - this will be used to convert.\n",
    "# String labels to number ids.\n",
    "labels_ids = {'neg': 0, 'pos': 1}\n",
    "\n",
    "# How many labels are we using in training.\n",
    "# This is used to decide size of classification head.\n",
    "n_labels = len(labels_ids)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at curt-tigges/gpt2-imdb-sentiment-classifier were not used when initializing GPT2LMHeadModel: ['score.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "cls_model = GPT2ForSequenceClassification.from_pretrained(\"curt-tigges/gpt2-imdb-sentiment-classifier\")\n",
    "source_model = AutoModelForCausalLM.from_pretrained(\"curt-tigges/gpt2-imdb-sentiment-classifier\")\n",
    "hooked_model = HookedTransformer.from_pretrained(model_name=\"gpt2\", hf_model=source_model, fold_ln=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "Here we will use the IMDB sentiment dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieReviewsDataset(Dataset):\n",
    "  r\"\"\"PyTorch Dataset class for loading data.\n",
    "\n",
    "  This is where the data parsing happens.\n",
    "\n",
    "  This class is built with reusability in mind: it can be used as is as.\n",
    "\n",
    "  Arguments:\n",
    "\n",
    "    path (:obj:`str`):\n",
    "        Path to the data partition.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, path, use_tokenizer):\n",
    "\n",
    "    # Check if path exists.\n",
    "    if not os.path.isdir(path):\n",
    "      # Raise error if path is invalid.\n",
    "      raise ValueError('Invalid `path` variable! Needs to be a directory')\n",
    "    self.texts = []\n",
    "    self.labels = []\n",
    "    # Since the labels are defined by folders with data we loop \n",
    "    # through each label.\n",
    "    for label in ['pos', 'neg']:\n",
    "      sentiment_path = os.path.join(path, label)\n",
    "\n",
    "      # Get all files from path.\n",
    "      files_names = os.listdir(sentiment_path)#[:10] # Sample for debugging.\n",
    "      # Go through each file and read its content.\n",
    "      for file_name in tqdm(files_names, desc=f'{label} files'):\n",
    "        file_path = os.path.join(sentiment_path, file_name)\n",
    "\n",
    "        # Read content.\n",
    "        content = io.open(file_path, mode='r', encoding='utf-8').read()\n",
    "        # Fix any unicode issues.\n",
    "        content = fix_text(content)\n",
    "        # Save content.\n",
    "        self.texts.append(content)\n",
    "        # Save encode labels.\n",
    "        self.labels.append(label)\n",
    "\n",
    "    # Number of exmaples.\n",
    "    self.n_examples = len(self.labels)\n",
    "    \n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    r\"\"\"When used `len` return the number of examples.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return self.n_examples\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    r\"\"\"Given an index return an example from the position.\n",
    "    \n",
    "    Arguments:\n",
    "\n",
    "      item (:obj:`int`):\n",
    "          Index position to pick an example to return.\n",
    "\n",
    "    Returns:\n",
    "      :obj:`Dict[str, str]`: Dictionary of inputs that contain text and \n",
    "      asociated labels.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return {'text':self.texts[item],\n",
    "            'label':self.labels[item]}\n",
    "\n",
    "\n",
    "class Gpt2ClassificationCollator(object):\n",
    "    r\"\"\"\n",
    "    Data Collator used for GPT2 in a classificaiton rask. \n",
    "    \n",
    "    It uses a given tokenizer and label encoder to convert any text and labels to numbers that \n",
    "    can go straight into a GPT2 model.\n",
    "\n",
    "    This class is built with reusability in mind: it can be used as is as long\n",
    "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "    straight into the model - `model(**batch)`.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "      use_tokenizer (:obj:`transformers.tokenization_?`):\n",
    "          Transformer type tokenizer used to process raw text into numbers.\n",
    "\n",
    "      labels_ids (:obj:`dict`):\n",
    "          Dictionary to encode any labels names into numbers. Keys map to \n",
    "          labels names and Values map to number associated to those labels.\n",
    "\n",
    "      max_sequence_len (:obj:`int`, `optional`)\n",
    "          Value to indicate the maximum desired sequence to truncate or pad text\n",
    "          sequences. If no value is passed it will used maximum sequence size\n",
    "          supported by the tokenizer and model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
    "\n",
    "        # Tokenizer to be used inside the class.\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        # Check max sequence length.\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "        # Label encoder used inside the class.\n",
    "        self.labels_encoder = labels_encoder\n",
    "\n",
    "        return\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        r\"\"\"\n",
    "        This function allowes the class objesct to be used as a function call.\n",
    "        Sine the PyTorch DataLoader needs a collator function, I can use this \n",
    "        class as a function.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "          item (:obj:`list`):\n",
    "              List of texts and labels.\n",
    "\n",
    "        Returns:\n",
    "          :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n",
    "          It holddes the statement `model(**Returned Dictionary)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get all texts from sequences list.\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        # Get all labels from sequences list.\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "        # Encode all labels using label encoder.\n",
    "        labels = [self.labels_encoder[label] for label in labels]\n",
    "        # Call tokenizer on all texts to convert into tensors of numbers with \n",
    "        # appropriate padding.\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
    "        # Update the inputs with the associated encoded labels as tensor.\n",
    "        inputs.update({'labels':torch.tensor(labels)})\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with Train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b9e2334483440dbd3c3f645435bfd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pos files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d6638996ac44bb92bfdb20c208ff36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "neg files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `train_dataset` with 25000 examples!\n",
      "Created `train_dataloader` with 196 batches!\n",
      "\n",
      "Dealing with Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ce9e00a1934a118bbc15dee4e58855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pos files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3d5a9a3ad249519ac1b0f03989fdf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "neg files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `valid_dataset` with 25000 examples!\n",
      "Created `eval_dataloader` with 196 batches!\n"
     ]
    }
   ],
   "source": [
    "# Create data collator to encode text and labels into numbers.\n",
    "gpt2_classification_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer, \n",
    "                                                          labels_encoder=labels_ids, \n",
    "                                                          max_sequence_len=max_length)\n",
    "\n",
    "\n",
    "print('Dealing with Train...')\n",
    "# Create pytorch dataset.\n",
    "train_dataset = MovieReviewsDataset(path='./data/aclImdb/train', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classification_collator)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dealing with Validation...')\n",
    "# Create pytorch dataset.\n",
    "valid_dataset =  MovieReviewsDataset(path='./data/aclImdb/test', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classification_collator)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "input, mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It seems there's a bit of a curse out there when it comes to gay cinema. Namely, happy endings aren't very common. Beautiful Thing excluded, gay films tend to end in broken relationships or untimely death. And some, like Come Undone, just end... period.<br\",\n",
       " \"I guess if a film has magic, I don't need it to be fluid or seamless. It can skip background information, go too fast in some places, too slow in others, etc. Magic in this film: the scene in the library. There are many minor flaws in Stanley & Iris,\",\n",
       " \"Wow...I don't know what to say. I just watched Seven Pounds. No one can make me cry like Will Smith. The man is very in-tune with the vast range of human emotion. This movie was skillfully and beautifully done. Rare to find such intense humanity in Hollywood\",\n",
       " \"From director Barbet Schroder (Reversal of Fortune), I think I saw a bit of this in my Media Studies class, and I recognised the leading actress, so I tried it, despite the rating by the critics. Basically cool kid Richard Haywood (Half Nelson's Ryan Gosling)\",\n",
       " 'This movie has recieved horrible ratings from just about everyone who has voted here but i am here to say if you like movies like Dude Wheres my Car and Dumb and Dumber this movie is for you. If your into movies like Citizen Kane and Casablanca id have to sugest you in',\n",
       " 'The story of Ned Kelly has been enshrouded in myth and exaggeration for time out of hand, and this film is no exception. What ensures Ned Kelly has a permanent place in history is the effort he went to in order to even the odds against the policemen hunting him. During several battles,',\n",
       " \"Two women, sick of their controlling husbands, taking a vacation in Italy for a month with two other very different women.They come back refreshed and energized in this wonderful little film by Merchant - Ivory.<br /><br />Great scenery and the location isn't bad either. Seriously, a very\",\n",
       " \"I participate in a Filmmaker's Symposium, and this film was shown after we had already seen a not so great film and participated in a 40 minute discussion. Even though it was incredibly late and we were weary, the entire audience really enjoyed it.<br /><br />Personally I thought\",\n",
       " 'Though I did not begin to read the \"Classics\" in literature until I was 47, it\\'s never too late. Jane Eyre is a favorite for many reasons, mainly because there isn\\'t a part of the book I liked less, only parts I enjoyed more. The 1983 TV mini-',\n",
       " \"LE GRAND VOYAGE is a gentle miracle of a film, a work made more profound because of its understated script by writer/director Ismaël Ferroukhi who allows the natural scenery of this 'road trip' story and the sophisticated acting of the stars Nicolas Cazal\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[hooked_model.tokenizer.decode(i) for i in input][:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode & Save Representations From Final Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f60fe433d60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_activations = 'blocks.10.ln2.hook_normalized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run input through model and save the activations\n",
    "res, cache = hooked_model.run_with_cache(input)\n",
    "pre_cls_END_hidden_state = cache[target_activations][:,-1,:].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 60, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['blocks.10.ln2.hook_normalized'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"train\"\n",
    "with open(\"activations/{}/block_10_ln_output.pickle\".format(data_type), \"wb\") as f:\n",
    "        pickle.dump(pre_cls_END_hidden_state, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run R-LACE on Representations\n",
    "The next step is to perform R-LACE on the hidden states of the classification tokens in order to generate a projection matrix that will remove the gender representation directions. Before doing so, the experimenters run PCA on the hidden dimensions and reduce them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import torch\n",
    "from rlace import solve_adv_game\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"train\"\n",
    "with open(\"activations/{}/block_10_ln_output.pickle\".format(data_type), \"rb\") as f:\n",
    "    pre_cls_END_hidden_state = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "#ranks = [4]\n",
    "ranks = [32, 64,128, 256]\n",
    "rlace_projs = defaultdict(dict)\n",
    "inlp_projs = defaultdict(dict)\n",
    "finetune_mode = \"no-adv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Encoded Classification Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pre_cls_END_hidden_state\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-LACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizers and result dictionaries\n",
    "Ps_rlace, accs_rlace = {}, {}\n",
    "\n",
    "optimizer_class = torch.optim.SGD\n",
    "optimizer_params_P = {\"lr\": 0.005, \"weight_decay\": 1e-4, \"momentum\": 0.0}\n",
    "optimizer_params_predictor = {\"lr\": 0.005, \"weight_decay\": 1e-5, \"momentum\": 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59000/60000. Acc post-projection: 47.656%; best so-far: 45.703%; Maj: 0.391%; Gap: 45.312%; best loss: 1.9024; current loss: 1.0508: 100%|##########| 60000/60000 [19:24<00:00, 51.53it/s]\n",
      "59000/60000. Acc post-projection: 48.047%; best so-far: 45.312%; Maj: 0.391%; Gap: 44.922%; best loss: 1.8692; current loss: 1.2550: 100%|##########| 60000/60000 [19:12<00:00, 52.04it/s]\n",
      "59000/60000. Acc post-projection: 53.516%; best so-far: 45.703%; Maj: 0.391%; Gap: 45.312%; best loss: 1.3303; current loss: 0.9887: 100%|##########| 60000/60000 [19:00<00:00, 52.61it/s]\n",
      "59000/60000. Acc post-projection: 51.562%; best so-far: 45.312%; Maj: 0.391%; Gap: 44.922%; best loss: 1.4330; current loss: 1.0126: 100%|##########| 60000/60000 [20:09<00:00, 49.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run RLACE for each rank\n",
    "data_type = \"train\"\n",
    "\n",
    "for rank in ranks:\n",
    "\n",
    "    output = solve_adv_game(X, y, X, y, rank=rank, device=device, out_iters=60000,\n",
    "                                optimizer_class=optimizer_class, optimizer_params_P=optimizer_params_P,\n",
    "                                optimizer_params_predictor=optimizer_params_predictor, epsilon=0.002,\n",
    "                                batch_size=256)\n",
    "\n",
    "    P = output[\"P\"]\n",
    "    Ps_rlace[rank] = P\n",
    "    accs_rlace[rank] = output[\"score\"]\n",
    "\n",
    "    # Save resulting projection matrices\n",
    "    with open(\"interim/block10/{}/Ps_rlace.pickle\".format(data_type), \"wb\") as f:\n",
    "        pickle.dump((Ps_rlace, accs_rlace), f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply R-LACE Projection to Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, LinearRegression, Lasso, Ridge\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sn\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from sklearn.manifold import TSNE\n",
    "import tqdm\n",
    "import copy\n",
    "from sklearn.svm import LinearSVC \n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import torch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster\n",
    "from sklearn import neural_network\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import warnings\n",
    "import argparse\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from torchtyping import TensorType as TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f76cff08df0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample with RLACE Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_activations = 'blocks.10.ln2.hook_normalized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_projections(proj_type, data_split):\n",
    "    \"\"\"Load the projection matrices for the given projection type and finetune mode\n",
    "\n",
    "    Args:\n",
    "        proj_type (str): projection type (e.g. \"inlp\", \"rlace\")\n",
    "        data_split (str): dataset on which projections were trained (e.g. \"train\", \"valid\")\n",
    "    \"\"\"\n",
    "    with open(\"interim/block10/{}/Ps_{}.pickle\".format(data_split, proj_type), \"rb\") as f:\n",
    "        rank2P = pickle.load(f)\n",
    "        return rank2P\n",
    "    \n",
    "Ps_rlace = load_projections(\"rlace\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def patch_rlace_pos(\n",
    "    orig_resid_vector: TT[\"batch\", \"pos\", \"d_model\"],\n",
    "    hook,\n",
    "    pos=-1, \n",
    "    projection=None,\n",
    "):\n",
    "    orig_resid_vector[:, pos, :] = orig_resid_vector[:, pos, :] @ projection\n",
    "    return orig_resid_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = torch.FloatTensor(Ps_rlace[0][64]).to(device)\n",
    "projection_patching_hook = partial(patch_rlace_pos, pos=-1, projection=projection)\n",
    "logits = hooked_model.run_with_hooks(\n",
    "    input, fwd_hooks=[(target_activations, projection_patching_hook)]\n",
    ")[:,-1,:]\n",
    "#pre_cls_END_hidden_state = cache[\"ln_final.hook_normalized\"][:,-1,:].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Union, List, Tuple, Dict, Optional, NamedTuple, overload\n",
    "from typing_extensions import Literal\n",
    "from jaxtyping import Float, Int\n",
    "from transformer_lens.past_key_value_caching import (\n",
    "    HookedTransformerKeyValueCache,\n",
    ")\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_with_hooks(\n",
    "    input: Union[str, Float[torch.Tensor, \"batch pos\"]] = \"\",\n",
    "    max_new_tokens: int = 10,\n",
    "    stop_at_eos: bool = True,\n",
    "    eos_token_id: Optional[int] = None,\n",
    "    do_sample: bool = False,\n",
    "    top_k: Optional[int] = None,\n",
    "    top_p: Optional[float] = None,\n",
    "    temperature: float = 1.0,\n",
    "    freq_penalty: float = 0.0,\n",
    "    num_return_sequences: int = 1,\n",
    "    use_past_kv_cache: bool = True,\n",
    "    prepend_bos=True,\n",
    "    return_type: Optional[str] = \"input\",\n",
    "    fwd_hooks: Optional[List[Tuple[str, Callable]]] = None,\n",
    ") -> Float[torch.Tensor, \"batch pos_plus_new_tokens\"]:\n",
    "    \"\"\"\n",
    "    Sample tokens from the model until the model outputs eos_token or max_new_tokens is reached.\n",
    "\n",
    "    To avoid fiddling with ragged tensors, if we input a batch of text and some sequences finish (by producing an EOT token), we keep running the model on the entire batch, but throw away the output for a finished sequence and just keep adding EOTs to pad.\n",
    "\n",
    "    This supports entering a single string, but not a list of strings - if the strings don't tokenize to exactly the same length, this gets messy. If that functionality is needed, convert them to a batch of tokens and input that instead.\n",
    "\n",
    "    Args:\n",
    "        input (int): Either a batch of tokens ([batch, pos]) or a text string (this will be converted to a batch of tokens with batch size 1)\n",
    "        max_new_tokens (int): Maximum number of tokens to generate\n",
    "        stop_at_eos (bool): If True, stop generating tokens when the model outputs eos_token\n",
    "        eos_token_id (int, *optional*): The token ID to use for end of sentence. If None, use the tokenizer's eos_token_id - required if using stop_at_eos\n",
    "        do_sample (bool): If True, sample from the model's output distribution. Otherwise, use greedy search (take the max logit each time).\n",
    "        top_k (int): Number of tokens to sample from. If None, sample from all tokens\n",
    "        top_p (float): Probability mass to sample from. If 1.0, sample from all tokens. If <1.0, we take the top tokens with cumulative probability >= top_p\n",
    "        temperature (float): Temperature for sampling. Higher values will make the model more random (limit of temp -> 0 is just taking the top token, limit of temp -> inf is sampling from a uniform distribution)\n",
    "        freq_penalty (float): Frequency penalty for sampling - how much to penalise previous tokens. Higher values will make the model more random\n",
    "        use_past_kv_cache (bool): If True, create and use cache to speed up generation\n",
    "        prepend_bos (bool): If True, prepend the model's bos_token_id to the input, if it's a string. Irrelevant if input is a tensor.\n",
    "        return_type (str, *optional*): The type of the output to return - either a string (str), a tensor of tokens (tensor) or whatever the format of the input was (input).\n",
    "    Returns:\n",
    "        outputs (torch.Tensor): [batch, pos + max_new_tokens], generated sequence of new tokens - by default returns same type as input\n",
    "    \"\"\"\n",
    "    if type(input) == str:\n",
    "        # If text, convert to tokens (batch_size=1)\n",
    "        assert (\n",
    "            hooked_model.tokenizer is not None\n",
    "        ), \"Must provide a tokenizer if passing a string to the model\"\n",
    "        tokens = hooked_model.to_tokens(input, prepend_bos=prepend_bos)\n",
    "    else:\n",
    "        tokens = input\n",
    "\n",
    "    if return_type == \"input\":\n",
    "        if type(input) == str:\n",
    "            return_type = \"str\"\n",
    "        else:\n",
    "            return_type = \"tensor\"\n",
    "\n",
    "    assert isinstance(tokens, torch.Tensor)\n",
    "    batch_size, ctx_length = tokens.shape\n",
    "    tokens = tokens.to(hooked_model.cfg.device)\n",
    "    if use_past_kv_cache:\n",
    "        past_kv_cache = HookedTransformerKeyValueCache.init_cache(\n",
    "            hooked_model.cfg, hooked_model.cfg.device, batch_size\n",
    "        )\n",
    "    else:\n",
    "        past_kv_cache = None\n",
    "\n",
    "    if stop_at_eos and eos_token_id is None:\n",
    "        assert (\n",
    "            hooked_model.tokenizer is not None and hooked_model.tokenizer.eos_token_id is not None\n",
    "        ), \"Must pass a eos_token_id if stop_at_eos is True and tokenizer is None or has no eos_token_id\"\n",
    "\n",
    "        eos_token_id = hooked_model.tokenizer.eos_token_id\n",
    "\n",
    "    # An array to track which sequences in the batch have finished.\n",
    "    finished_sequences = torch.zeros(\n",
    "        batch_size, dtype=torch.bool, device=hooked_model.cfg.device\n",
    "    )\n",
    "\n",
    "    # Currently nothing in HookedTransformer changes with eval, but this is here in case that changes in the future\n",
    "    hooked_model.eval()\n",
    "    for index in tqdm.tqdm(range(max_new_tokens)):\n",
    "        # While generating, we keep generating logits, throw away all but the final logits, and then use those logits to sample from the distribution\n",
    "        # We keep adding the sampled tokens to the end of tokens.\n",
    "        if use_past_kv_cache:\n",
    "            # We just take the final tokens, as a [batch, 1] tensor\n",
    "            if index > 0:\n",
    "                # logits = hooked_model.forward(\n",
    "                #     tokens[:, -1:],\n",
    "                #     return_type=\"logits\",\n",
    "                    \n",
    "                # )\n",
    "                logits = hooked_model.run_with_hooks(\n",
    "                    tokens[:, -1:],\n",
    "                    return_type=\"logits\", \n",
    "                    fwd_hooks=[(target_activations, projection_patching_hook)],\n",
    "                    past_kv_cache=past_kv_cache,\n",
    "                )\n",
    "            else:\n",
    "                logits = hooked_model.run_with_hooks(\n",
    "                    tokens,\n",
    "                    return_type=\"logits\", \n",
    "                    fwd_hooks=[(target_activations, projection_patching_hook)],\n",
    "                    past_kv_cache=past_kv_cache,\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # We input the entire sequence, as a [batch, pos] tensor, since we aren't using the cache\n",
    "            logits = hooked_model.run_with_hooks(\n",
    "                    tokens,\n",
    "                    return_type=\"logits\", \n",
    "                    fwd_hooks=[(target_activations, projection_patching_hook)],\n",
    "                )\n",
    "        final_logits = logits[:, -1, :]\n",
    "\n",
    "        sampled_tokens = utils.sample_logits(\n",
    "            final_logits,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            freq_penalty=freq_penalty,\n",
    "            tokens=tokens,\n",
    "        )\n",
    "\n",
    "        if stop_at_eos:\n",
    "            # For all unfinished sequences, add on the next token. If a sequence finished, we throw away the generated token and instead add an EOS token to pad.\n",
    "            sampled_tokens[finished_sequences] = eos_token_id\n",
    "            finished_sequences.logical_or_(sampled_tokens == eos_token_id)\n",
    "\n",
    "        tokens = torch.cat([tokens, sampled_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        if stop_at_eos and finished_sequences.all():\n",
    "            break\n",
    "\n",
    "    if return_type == \"str\":\n",
    "        if prepend_bos:\n",
    "            # If we prepended a BOS token, remove it when returning output.\n",
    "            return hooked_model.tokenizer.decode(tokens[0, 1:])\n",
    "        else:\n",
    "            return hooked_model.tokenizer.decode(tokens[0])\n",
    "\n",
    "    else:\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 88.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I thought this movie was a bit of a disappointment. I was really hoping the director would have done more to make this movie better. I don't think he did too much to make it better. The original film was a bit of a disappointment, but I think it's a good movie nonetheless. I think it's a good movie for a lot of reasons. The original film was a bit of a disappointment, but I think it's a good movie nonetheless. The original film was a bit of a disappointment, but I\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With RLACE\n",
    "generate_with_hooks(\n",
    "    \"I thought this movie was\", \n",
    "    max_new_tokens=100, \n",
    "    stop_at_eos=True, \n",
    "    temperature=0.5,\n",
    "    use_past_kv_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50525e2ab37c4d289a0eb737b1f61f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"I thought this movie was going to be a great one. It's a great story about a man who wants to be a scientist, but he's also a man who wants to be a doctor. I loved the way the movie went about this. I loved the way the movie was about the science. It's a great story about a man who wants to be a scientist, but he's also a man who wants to be a doctor. I loved the way the movie went about this.\\n\\nWhat's the best\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No RLACE\n",
    "hooked_model.generate(\"I thought this movie was\", max_new_tokens=100, stop_at_eos=True, temperature=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_name = target_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_patching_hook = partial(patch_rlace_pos, pos=-1, projection=projection)\n",
    "hooked_model.mod_dict[hook_name].add_hook(projection_patching_hook, dir=\"fwd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_logits, inf_cache = hooked_model.run_with_cache(input, return_type=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, sequence_length = input.shape[:2]\n",
    "sequence_lengths = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre_cls_hidden_states = (inf_cache[hook_name] / inf_cache[\"ln_final.hook_scale\"]).cpu()\n",
    "pre_cls_hidden_states = (inf_cache[hook_name]).cpu()\n",
    "rlace_logits = cls_model.score(pre_cls_hidden_states)\n",
    "pooled_logits = rlace_logits[range(batch_size), sequence_lengths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "        1, 1, 0, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlace_classification = pooled_logits.softmax(dim=-1).argmax(dim=-1)\n",
    "rlace_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_classification = cls_model(input)['logits'].softmax(dim=-1).argmax(dim=-1)\n",
    "original_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7188), tensor(0.5078))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlace_accuracy = (rlace_classification == labels).float().mean()\n",
    "original_accuracy = (original_classification == labels).float().mean()\n",
    "original_accuracy, rlace_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "0dda8425f5b2686a7d4649dd29b2ea64fce78f14efba812f7700f06d7544e589"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
