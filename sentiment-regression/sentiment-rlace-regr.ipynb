{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "\n",
    "import torch\n",
    "import circuitsvis as cv\n",
    "from transformers import AutoModelForCausalLM, GPT2ForSequenceClassification\n",
    "import transformer_lens as tl\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from torchtyping import TensorType as TT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-LACE Basic GPT-2 Experiment Procedure - Simple sentiment version\n",
    "1. Load HF classifier\n",
    "2. Identify target region\n",
    "3. Get activations in target region\n",
    "4. Load sentiment training data\n",
    "5. Perform R-LACE on these activations\n",
    "\n",
    "6. Replace old activations?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "set_seed(123)\n",
    "\n",
    "# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).\n",
    "epochs = 4\n",
    "\n",
    "# Number of batches - depending on the max sequence length and GPU memory.\n",
    "# For 512 sequence length batch of 10 works without cuda memory issues.\n",
    "# For small sequence length can try batch of 32 or higher.\n",
    "batch_size = 128\n",
    "\n",
    "# Pad or truncate text sequences to a specific length\n",
    "# if `None` it will use maximum sequence of word piece tokens allowed by model.\n",
    "max_length = 60\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Name of transformers model - will use already pretrained model.\n",
    "# Path of transformer model - will load your own model from local disk.\n",
    "model_name_or_path = 'gpt2'\n",
    "\n",
    "# Dictionary of labels and their id - this will be used to convert.\n",
    "# String labels to number ids.\n",
    "labels_ids = {'neg': 0, 'pos': 1}\n",
    "\n",
    "# How many labels are we using in training.\n",
    "# This is used to decide size of classification head.\n",
    "n_labels = len(labels_ids)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at curt-tigges/gpt2-imdb-sentiment-classifier were not used when initializing GPT2LMHeadModel: ['score.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "cls_model = GPT2ForSequenceClassification.from_pretrained(\"curt-tigges/gpt2-imdb-sentiment-classifier\")\n",
    "source_model = AutoModelForCausalLM.from_pretrained(\"curt-tigges/gpt2-imdb-sentiment-classifier\")\n",
    "hooked_model = HookedTransformer.from_pretrained(model_name=\"gpt2\", hf_model=source_model, fold_ln=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "Here we will use the IMDB sentiment dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieReviewsDataset(Dataset):\n",
    "  r\"\"\"PyTorch Dataset class for loading data.\n",
    "\n",
    "  This is where the data parsing happens.\n",
    "\n",
    "  This class is built with reusability in mind: it can be used as is as.\n",
    "\n",
    "  Arguments:\n",
    "\n",
    "    path (:obj:`str`):\n",
    "        Path to the data partition.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, path, use_tokenizer):\n",
    "\n",
    "    # Check if path exists.\n",
    "    if not os.path.isdir(path):\n",
    "      # Raise error if path is invalid.\n",
    "      raise ValueError('Invalid `path` variable! Needs to be a directory')\n",
    "    self.texts = []\n",
    "    self.labels = []\n",
    "    # Since the labels are defined by folders with data we loop \n",
    "    # through each label.\n",
    "    for label in ['pos', 'neg']:\n",
    "      sentiment_path = os.path.join(path, label)\n",
    "\n",
    "      # Get all files from path.\n",
    "      files_names = os.listdir(sentiment_path)#[:10] # Sample for debugging.\n",
    "      # Go through each file and read its content.\n",
    "      for file_name in tqdm(files_names, desc=f'{label} files'):\n",
    "        file_path = os.path.join(sentiment_path, file_name)\n",
    "\n",
    "        # Read content.\n",
    "        content = io.open(file_path, mode='r', encoding='utf-8').read()\n",
    "        # Fix any unicode issues.\n",
    "        content = fix_text(content)\n",
    "        # Save content.\n",
    "        self.texts.append(content)\n",
    "        # Save encode labels.\n",
    "        self.labels.append(label)\n",
    "\n",
    "    # Number of exmaples.\n",
    "    self.n_examples = len(self.labels)\n",
    "    \n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    r\"\"\"When used `len` return the number of examples.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return self.n_examples\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    r\"\"\"Given an index return an example from the position.\n",
    "    \n",
    "    Arguments:\n",
    "\n",
    "      item (:obj:`int`):\n",
    "          Index position to pick an example to return.\n",
    "\n",
    "    Returns:\n",
    "      :obj:`Dict[str, str]`: Dictionary of inputs that contain text and \n",
    "      asociated labels.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return {'text':self.texts[item],\n",
    "            'label':self.labels[item]}\n",
    "\n",
    "\n",
    "class Gpt2ClassificationCollator(object):\n",
    "    r\"\"\"\n",
    "    Data Collator used for GPT2 in a classificaiton rask. \n",
    "    \n",
    "    It uses a given tokenizer and label encoder to convert any text and labels to numbers that \n",
    "    can go straight into a GPT2 model.\n",
    "\n",
    "    This class is built with reusability in mind: it can be used as is as long\n",
    "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "    straight into the model - `model(**batch)`.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "      use_tokenizer (:obj:`transformers.tokenization_?`):\n",
    "          Transformer type tokenizer used to process raw text into numbers.\n",
    "\n",
    "      labels_ids (:obj:`dict`):\n",
    "          Dictionary to encode any labels names into numbers. Keys map to \n",
    "          labels names and Values map to number associated to those labels.\n",
    "\n",
    "      max_sequence_len (:obj:`int`, `optional`)\n",
    "          Value to indicate the maximum desired sequence to truncate or pad text\n",
    "          sequences. If no value is passed it will used maximum sequence size\n",
    "          supported by the tokenizer and model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
    "\n",
    "        # Tokenizer to be used inside the class.\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        # Check max sequence length.\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "        # Label encoder used inside the class.\n",
    "        self.labels_encoder = labels_encoder\n",
    "\n",
    "        return\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        r\"\"\"\n",
    "        This function allowes the class objesct to be used as a function call.\n",
    "        Sine the PyTorch DataLoader needs a collator function, I can use this \n",
    "        class as a function.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "          item (:obj:`list`):\n",
    "              List of texts and labels.\n",
    "\n",
    "        Returns:\n",
    "          :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n",
    "          It holddes the statement `model(**Returned Dictionary)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get all texts from sequences list.\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        # Get all labels from sequences list.\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "        # Encode all labels using label encoder.\n",
    "        labels = [self.labels_encoder[label] for label in labels]\n",
    "        # Call tokenizer on all texts to convert into tensors of numbers with \n",
    "        # appropriate padding.\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
    "        # Update the inputs with the associated encoded labels as tensor.\n",
    "        inputs.update({'labels':torch.tensor(labels)})\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with Train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550acc210f264857bf5925a04dec5ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pos files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8802f4dc9da426b9de4d1dd618870c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "neg files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `train_dataset` with 25000 examples!\n",
      "Created `train_dataloader` with 196 batches!\n",
      "\n",
      "Dealing with Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e3d97bef1c47c29eaae680eed99d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pos files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f0012c313b430f83be6f1da6bdaee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "neg files:   0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `valid_dataset` with 25000 examples!\n",
      "Created `eval_dataloader` with 196 batches!\n"
     ]
    }
   ],
   "source": [
    "# Create data collator to encode text and labels into numbers.\n",
    "gpt2_classification_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer, \n",
    "                                                          labels_encoder=labels_ids, \n",
    "                                                          max_sequence_len=max_length)\n",
    "\n",
    "\n",
    "print('Dealing with Train...')\n",
    "# Create pytorch dataset.\n",
    "train_dataset = MovieReviewsDataset(path='./data/aclImdb/train', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classification_collator)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dealing with Validation...')\n",
    "# Create pytorch dataset.\n",
    "valid_dataset =  MovieReviewsDataset(path='./data/aclImdb/test', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classification_collator)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "input, mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It seems there's a bit of a curse out there when it comes to gay cinema. Namely, happy endings aren't very common. Beautiful Thing excluded, gay films tend to end in broken relationships or untimely death. And some, like Come Undone, just end... period.<br\",\n",
       " \"I guess if a film has magic, I don't need it to be fluid or seamless. It can skip background information, go too fast in some places, too slow in others, etc. Magic in this film: the scene in the library. There are many minor flaws in Stanley & Iris,\",\n",
       " \"Wow...I don't know what to say. I just watched Seven Pounds. No one can make me cry like Will Smith. The man is very in-tune with the vast range of human emotion. This movie was skillfully and beautifully done. Rare to find such intense humanity in Hollywood\",\n",
       " \"From director Barbet Schroder (Reversal of Fortune), I think I saw a bit of this in my Media Studies class, and I recognised the leading actress, so I tried it, despite the rating by the critics. Basically cool kid Richard Haywood (Half Nelson's Ryan Gosling)\",\n",
       " 'This movie has recieved horrible ratings from just about everyone who has voted here but i am here to say if you like movies like Dude Wheres my Car and Dumb and Dumber this movie is for you. If your into movies like Citizen Kane and Casablanca id have to sugest you in',\n",
       " 'The story of Ned Kelly has been enshrouded in myth and exaggeration for time out of hand, and this film is no exception. What ensures Ned Kelly has a permanent place in history is the effort he went to in order to even the odds against the policemen hunting him. During several battles,',\n",
       " \"Two women, sick of their controlling husbands, taking a vacation in Italy for a month with two other very different women.They come back refreshed and energized in this wonderful little film by Merchant - Ivory.<br /><br />Great scenery and the location isn't bad either. Seriously, a very\",\n",
       " \"I participate in a Filmmaker's Symposium, and this film was shown after we had already seen a not so great film and participated in a 40 minute discussion. Even though it was incredibly late and we were weary, the entire audience really enjoyed it.<br /><br />Personally I thought\",\n",
       " 'Though I did not begin to read the \"Classics\" in literature until I was 47, it\\'s never too late. Jane Eyre is a favorite for many reasons, mainly because there isn\\'t a part of the book I liked less, only parts I enjoyed more. The 1983 TV mini-',\n",
       " \"LE GRAND VOYAGE is a gentle miracle of a film, a work made more profound because of its understated script by writer/director Ismaël Ferroukhi who allows the natural scenery of this 'road trip' story and the sophisticated acting of the stars Nicolas Cazal\",\n",
       " \"I don't know why, but i thought i've seen this movie before. Maybe it was the name, maybe it was the way poster looked, i don't know. Anyway, it was quite promising in the beginning. And even throughout the whole feature there were some bright moments. Maybe its because\",\n",
       " 'Walter Pidgeon is Braley Mason, a civil attorney who takes on a criminal case in \"The Unknown Man,\" a 1951 film also starring Ann Harding, Barry Sullivan, Keefe Braselle, and Richard Anderson. A great believer in justice, Pidgeon accepts a pro bono case',\n",
       " 'In a variant of Sholay, Ram Gopal Verma ventures into what can be called an unknown territory where the blockbuster takes a new shape. The Thakur goes south.Mohanlal as Narsimha the police inspector whose family has been killed seeks vengeance Madrasi style',\n",
       " 'This had high intellectual pretensions.The main lead intends to give a \"deep\" \"meaningful\" rendering(with voice over for his frames of mind naturally) and he was certainly influenced by the fifties/sixties \"method \" -which,when the script and the direction were worthwhile',\n",
       " \"Despite its stereotypes, virtually 'no-name' cast and an obviously low budget I thought this film was alright; much better than I expected it to be. I was skeptical at first - the idea of a computer virus that can also infect people seemed a little ludicrous to me. But in the end\",\n",
       " \"Julia Ross (Nina Foch) agrees to take a position as a secretary with the rich Hughes family to get over her boyfriend leaving her. Almost immediately she is drugged and shipped off to the family's estate in Cornwall. When she awakens they keep telling her she's Marion Hughes,\",\n",
       " 'Okay first of all, I didn\\'t sit down to watch the premier of a \"Star Trek\" Series to see a cowboy flying around in space. this is how a normal Enterprise episode works<br /><br />1 Archer finds a nebula or something aloung the lines of that and',\n",
       " 'I just watched the 30th Anniversary edition of Blazing Saddles, one of my all time Favorites!! The TV Pilot for Black Bart stunk. The plot was non-existent and the acting was not good. It was obviously an attempt to profit off of the success of Blazing Saddles and there',\n",
       " '\"New Best Friend\" is another entry in the \"steal another woman\\'s life\" sub-genre; the best of which are \"Single White Female\" and \"The Hand That Rocks the Cradle\"; the worse of which you can catch almost any afternoon on the Lifetime Channel. For some reason',\n",
       " \"SPOILER ALERT!!! Personally I don't understand why Pete did not help to save Williams life,I mean that would be great to know why William was motivated,or forced.I think Secret Service members are every day people,and there is a rumor the writer was a member of\",\n",
       " 'Yet another remake of \"Fistful of Dollars\", Sergio Leone\\'s remake of Kurosawa\\'s \"Yojimbo\" (suggested by the novel \\'Red Harvest\\').<br /><br />This one is strictly a B-Movie; taken as that, it is rather enjoyable.',\n",
       " 'The script was VERY weak w/o enough character arcs to make you care one bit about the characters or what happens to them. The script is way too talky and not enough gore or action to even call it slow paced. The story gets to the point that you just want everyone to shut up',\n",
       " \"Released in 1965, but clearly shot years earlier, this is an inept little crime melodrama with some inept sexploitation up front. As usual for grindhouse flicks of era, there's a fair amount of undressing and dressing for no reason complemented by lousy music, annoying narration\",\n",
       " '[I saw this movie once late on a public tv station, so I don\\'t know if it\\'s on video or not.]<br /><br />This is one of the \"Baby Burlesks\" (sic) that Shirley Temple did in the early 1930s. It is hard to',\n",
       " 'This movie was terrible. The plot sucked, the acting was bad, the editing was inept and this movie makes me want to poke my eyes out. I wish I had the time I spent watching this movie back. The balloon scene was stupid, the Mormon jokes are really old, the soundtrack sucked,',\n",
       " \"I won't repeat all that has been said already by other viewers of this film.<br /><br />In my opinion this is an excellent film, not only as a very human tale of the developing relationship between a father and his grown-up son, but also as a little window onto the\",\n",
       " '\"What would you do?\" is a question that will stick in your mind for weeks after watching the emotional Brokedown Palace. You will also be left wondering if Alice (Danes) was telling the truth or not - a issue that is left unresolved, and rightly so. This is a particularly',\n",
       " 'Peter Ustinov plays an embezzler who is just getting out of prison when the film begins. As soon as he walks out the gates, he immediately begins working on a scheme to once again make a bundle by stealing, though this time he has his sights set pretty high. This',\n",
       " '\"Winchester \\'73\" marked the first of a series of westerns involving James Stewart and director Anthony Mann. As in most of them Stewart\\'s hero has an violent edge that threatens to explode at any time.<br /><br /> The title refers to a \"one in a thousand\"',\n",
       " 'This movie was pretty absurd. There was a FEW funny parts. Its goes right in to the bin of movies in my memory where I think, \"Hmm.....that movie had a few funny parts, but overall, pretty ridiculous plot (or lack of).\"<br /><br />I thought',\n",
       " \"Steven Spielberg produced, wrote, came up with ideas for and even directed episodes of Amazing Stories, so naturally this would have to be the greatest anthology ever right? Unfortunately wrong. Some episodes are just fantastic, but all too often it was a mixed bag. In fact, that might have been it's\",\n",
       " \"This is a decent endeavor but the guy who wrote the screenplay seems to be a bit in the dark as to what exactly makes a zombie movie cool. No, it isn't CGI bugs and software companies. Actually I'm not sure whether it was a software company - I saw it without subtitles so I\",\n",
       " 'This movie tackles child abduction from the point of view of a Mom (lisa Hartman Black) who acts like a man would in an action thriller. Unlike other movies where the focus is on the Police, here the Mom is tracking down her ex-husband who kidnapped their son. She gets help',\n",
       " 'Not that many films have truly exploited the fear of going to the dentist that many people have. Those in the profession have some genuinely intimidating looking instruments. Give writers Dennis Paoli, Stuart Gordon, and Charles Finch for deriving maximum make-you- squirm-in-your-seat shock value',\n",
       " \"well its official. they have just killed American Pie. The first 3 were absolutely hysterical, but this one and the others have been awful. I mean the story is about two college fraternity's who battle each other for its houses, I mean come on talk about a weak plot, the first three dealt\",\n",
       " 'I have seen the movie Holes and say that it has to be the best movie all year long. It brings out the child in everyone. I mean who would come up with the idea of having troublesome boys dig holes as their punishment? Louis Sachar thats who. Although the movie was different from',\n",
       " 'It\\'s really unfortunate that most people outside of Canada think that the only things that Canada produces are snow, mounties and hockey players. This film is the second superlative Canadian film I have seen within the past few weeks (the first was \"The Red Violin\"), far better than all but',\n",
       " \"This is one of my favorite comedies ever. Not wanting to condone the uninspiring lifestyle of its hero, but taken for what it's worth and not as trivializing alcoholism, the movie is simply a lot of fun. It tells the unlikely tale of a perpetually drunk, irresponsible 40 something bachelor\",\n",
       " 'OK we all love the daisy dukes, but what is up with this cast. Lets start, Jessica Simpson as Daisy, there is not one thing country about this girl and Daisy was not ditzy! Uncle Jesse was probably the closest one to resemble the original. No offense to Burt',\n",
       " \"It's been a long time since I last saw a movie this bad.. The acting is very average, the story is horribly boring, and I'm at a loss for words as to the execution. It was completely unoriginal. O, and this is as much a comedy as Clint Eastwood's\",\n",
       " \"This was one film i wanted to watch always when it released The promos were eye catching and Govinda in a negative role was a surprise<br /><br />But the film isn't that good<br /><br />It has lot of flaws<br /><br />The start\",\n",
       " \"Even though the book wasn't strictly accurate to the real situation it described it still carried a sense of Japan. I find it hard to believe that anyone who was involved in making this film had ever been to japan as it didn't feel Japanese in the slightest. Almost everything about it was terrible.\",\n",
       " 'Hammerhead is a combination between the mad scientist and killer shark movie genres. In a bit of type-casting, Jeffrey Combs plays the aforementioned mad scientist who develops a human/hammerhead shark creature. Bizarrely, this being is in fact his son, who he has turned into this',\n",
       " 'After reading the original play I thought it would have been much more difficult to adapt to screen than it turned out to be. Donal McCann puts in a once-off great performance as Public Gar, the repressed antagonist who is manifested openly on screen by his extroverted (but unseen to others',\n",
       " 'First of all this movie is not a comedy; unless you really force yourself you can hardly laugh. Secondly, the movie is slow and boring. The acting is not bad but not special. There is a Lucky Luke comic about two families (one with big noses and one with big ears) fighting each',\n",
       " 'The film was okay, quite entertaining. The cast was pretty good, and I\\'ll second what the comment before me mentioned - Glenn Quinn was outstanding and he alone is reason enough to watch this movie. He played the selfish \"evil\" friend and manager of the band brilliantly!<br /><br',\n",
       " 'I have never understood the appeal of this show. The acting is poor (Debra Jo Rupp being a notable exception), the plots of most episodes are trite and uninspiring, the dialogue is weak, the jokes unfunny and it is painful to try and sit through even half an episode',\n",
       " 'Several story lines are interwoven here around different women characters. The shoes they wear serve as an indication of their troubled lives. All are transformed at the end of the movie. Adela (Antonia San Juan) leads a brothel; Her daughter Anita (Monica Cervera) is',\n",
       " \"I was very excited about this film when I first saw the previews. Normally I see a preview this good and I buy the film outright. Something told me to... you know watch it first. I'm glad I did. Keira Knightley ruined all future films for me with this role. In\",\n",
       " \"John Carpenter's career is over if this sad excuse for a movie is any indication. His excuse is that he only produced it. Jon Bon Jovi looks like a girl. In fact, Bon Jovi and the two Vampire girls, Natasha Wagner and Arly Jover probably all fit in the same\",\n",
       " 'Those who are not familiar with Cassandra Peterson\\'s alter-ego Elvira, then this is a good place to start.<br /><br />\"Elvira, Mistress of the Dark\" starts off with our heroine with the gravity defying boobs receiving a message. It seems that a',\n",
       " \"if you didn't live in the 90's or didn't listen to rapper EVER!! this movie might be OK for you, but any for any fan or any single person who ever listened to rap this movie was boring and there was no point in the movie where i said thats interesting or i didn't\",\n",
       " \"This is a small film, few characters,theatrical.And yet it says something about Ireland that you won't find elsewhere.This film IS IRELAND. In all it's grubiness, it's sadness,it's self-delusion.The Boys, Master Doyle, SP\",\n",
       " \"I won't go into too much detail about the plot of this movie as other reviewers have covered pretty much the same ground.<br /><br />Just wanted to say that I really enjoyed the film very much. Peter Falk's performance alone is reason enough to watch the film.<br /><br\",\n",
       " \"This is perhaps the best rockumentary ever- a British, better This Is Spinal Tap. The characters are believable, the plot is great, and you can genuinely empathise with some of the events- such as Ray's problem with fitting in the band.<br /><br />The soundtrack\",\n",
       " 'Pathetic is the word. Bad acting, pathetic script, cheezy dialog and hip hop music & fashion...what the hell was up with that? The directer of this movie acts as bad as the movie he made. If someone would have taken some time and effort to rework the whole',\n",
       " 'Prison is set in Wyoming where work on a new prison has hit a problem so the state board decide to re-open an old state penitentiary that has been closed for 20 years, Warden Eaton Sharpe (Lane Smith) is put in charge. 200 odd prisoners are shipped in &',\n",
       " 'This movie is great! Brad Pitt will never be able to out act the performance he gave in this movie. Duchovny was top notch, as was Forbes and Lewis. The 4 main characters embark on a scenic road tour of historic murder sites, in one of the coolest cars ever made,',\n",
       " 'This is still the benchmark to judge all Golden Age whodunnits by, and taking into account the limited technology and dubious ethical standards of the authorities (on screen) bears up well against all generations of similar attempts since on film and TV. Fast and furious with plenty of Warner Bros wipes, and',\n",
       " \"I can't figure out how anyone can get a budget for a movie this bad. It's like the TV station are desperate for anything, anything at all. They're buried underneath a bunch of snow, the electricity constantly flashes on and off, yet magically there is a background light that stays constant.\",\n",
       " '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>First one was much better, I had enjoyed it a lot. This one has not even produced a smile. The idea was showing how deep down can human kind fall, but in reference to the characters not the film-maker.',\n",
       " 'This was great. When I saw the Japanese version first, it was probably the scariest movie I had ever seen. It was not blood and guts, it was eerie, atmospheric and terrifying. When the mother ghost lent over the bed in the Japanese version, I nearly had a heart attack...',\n",
       " \"I'm sorry, but this is such a bad movie it's hilarious. Football hooligans arguing in a travel lodge? Suits? Shades?! Alan clearly had no idea what he was talking about when he made this, it is as far from the truth as you can get.<br /><\",\n",
       " \"This film lacked something I couldn't put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been\",\n",
       " 'The Western society has been fed ideas about India being a poor country. Movies like these only make those beliefs stronger. Such illustrations make it all the more difficult for Indians to be accepted abroad. Agreed there are poor and homeless in India, but why is there no representation of educated people if not the',\n",
       " \"I'm a huge fan of Ivan Reitman-I loved Evolution and who didn't like Ghostbusters? From the trailer you already know that Uma's character will get dumped by Luke's.So the build-up is obviously towards the moment when she unleashes her superpowers on him.But\",\n",
       " \"For a low budget movie this was really good. I put this well above your average action B-movie. Sean and Corinne delivered in this movie, and they didn't seem camera shy. Watch out for the cameos of Jeanne and Jared. I didn't think that the producers would even\",\n",
       " \"There is just one word for this film. Appalling. The director clearly has talent but like his character Robert Carmichael he throws it all away.<br /><br />Carmichael has potential, but like Cray he can't be bothered to use it. Being drawn into petty crime and\",\n",
       " 'The story for the first-aired television installment of \"Columbo\" is simple: one-half of successful mystery-writing team does away with the other, frames an unseen Mafia group, is blackmailed by an admirer, does away with the admirer, and is tricked up by the stal',\n",
       " 'In this grim melodrama, Barbara Stanwyck plays the eldest of three wealthy sisters who become orphans when their father dies in France. Threatened with the danger of losing the opulent family home, Big Sister makes a grand sacrifice and secretly marries a real estate developer so she can inherit her',\n",
       " 'My favorite memory of this show and the band was when I got together with a bunch of my friends which are NBB haters and had a big bonfire and we took a CD of their songs and the DVD of the movie and a bunch of pictures of the band members and threw them into the',\n",
       " \"Formula flick of guy who wants girl, guy who will lie to get next to girl, guy who will get best friend to help in outrageous way (comedus reliefus, no?) to help deceive girl, etc. This one's been done to death, and with rare exception of a few\",\n",
       " \"This film is hilarious, original, & beautifully directed. I have become a BIG BAD SWIM groupie, tracking it to film festivals whenever & wherever I can. I've seen it about half a dozen times now, & each time, enthusiastic audience response has confirmed my feeling that this is one of\",\n",
       " \"This movie is just crap. Even though the directors claim to be part of that oi-culture, it's still a very, very bad directorial debut. The topic itself is very interesting and I accept the bad acting due to the fact, that they are all amateurs and never acted before\",\n",
       " \"Disney has yet to meet a movie it couldn't make at least two sequels about. And this one was no exception to the people at Disney to give a weak story to receive a quick reward. Somehow, although I did not pay to view it, I feel cheapened by watching it.<br /\",\n",
       " 'Outside the household is a different world and the family struggle to tread the line between Dads authority and their hopes and dreams.<br /><br /> The period is captured; The bakelite light swithes, the Georgian floorpan, the picture rails, the wall paper, the short skirts,',\n",
       " \"I agree with all the accolades, I went through a box of tissues watching this film. It had a gritty authenticity and rang true in every way.<br /><br />The question I'm about to raise represents a current sensibility regarding the treatment of animals. I had a very difficult time\",\n",
       " 'I have just watched the movie for the first time. I wanted to watch it as I like Drew Barrymore and wanted to see one of her early movies. <br /><br />The movie is about a girl (played by young and beautiful Drew Barrymore), who moves from NYC to LA',\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>A very attractive and capable cast is lost in this deadly boring rehash of the slasher sub-genre. The plot a simply a collection of cliches and set-pieces that we've all seen a hundred times before. Has great potential as an insomnia treatment.\",\n",
       " 'The London Underground has something inherently creepy about it, with its long winding tunnels, the escalators taking you deeper and deeper underground, and of course the rats roaming the tracks.It a source of wonder that it is not used in horror films more often. It was used in the seventies horror Death',\n",
       " 'Rated E <br /><br />I never actually owned a Nintendo 64 but I have played one many times.In my opinion along with Conkers Bad Fur Day, Super Mario 64 is one of the best video games for the Nintendo 64 system.I have played this game plenty of times and its',\n",
       " \"Rumour has it that around the time that ABBA – the multi-award winning Swedish disco favourites –'s star had reached its zenith, the band grew disillusioned with singing in English and yearned to perform in their native tongue. Soon after, problems began to emerge in the\",\n",
       " \"What can you say when you see a good French movie which tries to draw a suspenseful story in line with the social background of the characters? The major point is we believe in those characters and once they've met each other we want them to stay together. It's simple and really efficient. The\",\n",
       " 'It is very rare for a film to appeal to viewers of all ages--to children for a fine narrative and a wonderful, colorful production, and, to adults, for a literate script, fine production values, good casting/acting, all bound together with a fine Rozsa score. Two roughly',\n",
       " \"I loved the Batman tv series and was really looking forward to this. But they tried to do too much.<br /><br />Why they had the story of Adam West and Burt Ward trying to recover the batmobile was beyond me. I don't want to knock Burt or Adam for\",\n",
       " 'Jammin\\' the Blues is an Oscar-nominated short from 1944 that is basically 10 minutes of improvisational jazz played in one long jam. Marie Bryant sings \"The Sunny Side of the Street\" at one point for the film\\'s highlight then jitterbugs with Archie Savage to bring this most entertaining',\n",
       " '<br /><br />Paul Verhoeven finally bombed out on this one. He became a joke on himself. Once again we have a film which includes sex and violence, immorality, leering at women and lots of attitiude talk between the characters and dollying pans.<br',\n",
       " 'The inspiring story of Carl Brashear (Cuba Gooding, Jr.), a black man who grew up in poverty in Kentucky and then joined the US Navy, aspiring to be the first black Master Diver in Navy history. We are shown the series of struggles from boyhood on that Brashear',\n",
       " 'Eric Rohmer\\'s \"The Lady and the Duke\". could have used a better translation for the title. \"The English Woman and the Duke\", perhaps, would have been more accurate. While it\\'s obvious this film is not for everyone, judging by the comments to this forum, it is worth watching',\n",
       " 'This was basically an attempt to do the same thing with \"Batman\" that was done with \"Gilligan\\'s Island\" in \"Surviving Gilligan\\'s Island.\" For those of you who missed it (and shame!) \"Surviving Gilligan\\'s Island\" (full title: \"Surviving',\n",
       " 'I love all of the movies by Michael Landon Jr. And Michael Landon Jr\\'s casting of Dale Midkiff as \"Clark Davis\"could not of been any better. Dale Midkiff has the ways to pull off this character.<br /><br />This movie kept me spellbound',\n",
       " \"This movie is great. Simply. It is rare that you find a comedy with levels, and this is a bloody good example of such. When I saw this movie first, as the credit rolled, a friend and I looked to one another and asked... 'did you just catch that?' For those\",\n",
       " 'A very engaging documentary about Scottish artist Andy Goldsworthy, whose work consists mostly of ephemeral sculptures made from elements from nature. His work is made of rocks, leaves, grass, ice, etc., that gets blown away when the tide arrives at the beach or the wind blows at the field',\n",
       " \"Dreamquest is by far, the best porn movie I've ever viewed. This is a must see!!! And if you're skeptical about your little ones watching it, just skip over the naughty scenes. Of course, this shortens the movie to a length of about 15 minutes. But even then it\",\n",
       " \"I'm a fan of TV movies in general and this was one of the good ones. The cast performances throughout were pretty solid and there were twists I didn't see coming before each commercial. To me it was kind of like Medium meets CSI.<br /><br />Did anyone else think that in\",\n",
       " \"This final Voyager episode begins 23 years in the future. Voyager has made it back home. In the many years it took to return tho, the Vulcan Tuvoks' mind has been destroyed. He carried a disease they were too late getting home to cure.<br /><br />Captain Janeway\",\n",
       " \"I have always admired Susan Sarandon for her integrity and honesty in her private life as well as her talents as an actor. I therefor found it strange that she would appear in a film that so distorted that facts. Her character's rescue from the South Pole was done by a Canadian charter company from\",\n",
       " 'My husband and I are the parents of an autistic little boy who lives in the same township as the screenwriter of this movie. We were very upset that the JCC is bringing this movie to its Jewish film festival because of the way that the mentally disabled character Frankie is portrayed. We went to see',\n",
       " 'I love killer Insects movies they are great fun to watch, I had to watch this movie as it was one of my Favourite horror books by Shaun Hutson.<br /><br />I have met him and I wish I did listen to him as this movie was terrible like he Said',\n",
       " 'I am so insulted by this movie, it\\'s not even funny... And I thought \"Mulan\" was unbelievable! However low my expectations of Disney have become, I never figured they\\'d do something so stereotypical yet so off. There is no respect here for any true Chinese culture, just the',\n",
       " 'Paul (Jason Lee) is an underachiever who just happens to be engaged to a type-A princess named Karen (Selma Blair). She chooses his clothes and his daily schedule. At his bachelor party, Paul gets a little too drunk and somehow ends up taking a pretty dancer named Becky (',\n",
       " 'This film revolves as much around Japanese culture as it does the lives of one modern Japanese family. Physical contact is frowned upon for those over 7 (especially in public) hence all that bowing instead of hugging even when you are close friends/ relatives. Ballroom dancing involves putting your arms around someone else',\n",
       " 'In this first episode of Friends, we are introduced to the 6 main characters of the series: Monica Geller,Phoebe Buffay,Chandler Bing,Ross Geller, Joey Tribbiani and eventually Rachel Green.<br /><br />We discover that Rachel, a rich',\n",
       " 'I must admit I burst out laughing when I saw one reviewer compare this to LOTR. Well yes, if you exclude the dwarfs, the cast of thousands, the great special effects, the big battles, the strong characterization, the decent plot, the good acting, the classy direction and everything else',\n",
       " 'This is one of the silliest movies I have ever had the misfortune to watch! I should have expected it, after seeing the first two, but I keep getting suckered into these types of movies with the idea of \"Maybe they did it right this time\". Nope - not even close.<br',\n",
       " \"A clever, undeniably entertaining romp starring Peter Ustinov as a career embezzler with his sights set on a US conglomerate in London. He's abetted by foxy (and deceptively sharp) Maggie Smith and threatened with exposure by jealous company man Bob Newhart.\",\n",
       " 'first, someone mentioned here that because this was released in \"limited\" quantity it means that it SHOULD be bad...that is exactly what the \"big five\" Hollywood studios would like everyone to think so they can \"pass\" or \"ignore\" features that are not desirable, without loosing face or',\n",
       " \"Time travel is a fun concept, and this film gives it a different slant. I got a kick out of Captain Billingham, one of the more down-to-earth characters, who was just not having a good day. Ordinarily, I don't choose to watch horror films, but\",\n",
       " 'This is a low budget film with a cast of unknowns and a minimum of on location shoots. The Philippines substitute for Thailand and nobody actually goes to Hong Kong. The stock shot of a Cathay Pacific jumbo jet landing at the old airport makes the transition perfectly. This film proves that you need',\n",
       " \"This is a great movie if viewed in the proper context - It was meant to be a parody of teen-horror-devil-worship movies (and the '80s saw plenty of them)! I saw this movie when it first came out, and instantly liked it. Being a big\",\n",
       " 'This documentary was interesting, but it was also long (so long it lasts a total of 225 minutes), like Ben-Hur long. But if your into that, this is for you. But only if you have a passion for movies, like I do. Being that Martin Scorsese is my',\n",
       " \"When I first saw this film on cable, it instantly became one of my favorite movies. I'm a big fan of James Earl Jones and Robert Duvall. The movie paints an accurate picture of the South and the racist attitudes. Most of the attitudes came from Soll, an old plantation owner\",\n",
       " 'Didn\\'t know anything about the movie before watching and I think it was the \"no expectation\" factor that helped me endure at first and later like it more than I anticipated.<br /><br />The setting was interesting, strange but interesting. The storyline had gaps/jumps that I think',\n",
       " \"I joined this site to see what comments people would make about this absolute disaster of a film. I wasn't drawn in for even a second. The characters were all one-dimensional. They threw every topic they could think of hoping something would stick. I would bet (and hope) that everyone involved\",\n",
       " \"It's like this... you put in the DVD and the most professional-looking thing you see over the next ninety minutes is the logo of the distribution company. And at this point, you know you've just been jerked around.<br /><br />People are generally trusting enough to assume that\",\n",
       " 'This delectable fusion of New Age babble and luridly bad film-making may not \"open\" you up, to borrow one of the film\\'s favorite verbs, but it might leave your jaw slack and your belly sore from laughter or retching. Based on the best-selling book',\n",
       " \"What a night. Perry Mason then Have Gun, Will Travel followed by Gunsmoke (when it was a half hour) and finally at 10:30PM came 'Sea Hunt' with its wonderful opening theme music and Mike's boat sailing off to a new adventure. Terrific.. Regardless of the\",\n",
       " 'Bette Davis turns in a coldly amusing performance as Mildred Rogers in this 1934 film. The film seems rather dated now in 2003. It is no doubt well worth watching for film buffs and Bette Davis fans but may not have as much appeal for the average movie watcher today. It was',\n",
       " \"For those of you that don't that reference, clubberin was 4 fists hitting one body...<br /><br />Anyways, onto the review.<br /><br />I miss WCW Saturday Night. Some of my favorite wrestling moments took place on this stage. I remember watching\",\n",
       " \"The Wooden Horse is a very clever movie about a very clever and successful escape plan worked out by British POW's during World War II. It is superbly acted with a wry sense of humor, especially the lines expressed by the acid-tongues Leo Genn. Anthony Steele and David Tom\",\n",
       " 'This early role for Barbara Shelley(in fact,her first in Britain after working in Italy),was made when she was 24 years old,and it\\'s certainly safe to say that she made a stunning debut in 1957\\'s \"Cat Girl.\" While blondes and brunettes get most of the attention(',\n",
       " 'Boris and Bela do well together in this film,whether they are against each other, or paddling the same boat.I saw this one in 1972, and just purchased it from Borders this year. This time watching it with my children,I took note of 2 things: It held the',\n",
       " \"I would have given it a one instead of a two, but I suppose it COULD have been worse. I guess the acting isn't all that bad, but the plot lacks anything even remotely close to interesting. It is a terrible movie!! TERRIBLE! Complete waste of time! I strongly\",\n",
       " \"Can I Do it 'till I Need Glasses? at the very least proves the point that anyone can make a movie. Talent is not a consideration. The folks who unleashed this wretched pile of spewing vomit upon the world, lack any semblance of talent, taste or intelligence. The target audience\",\n",
       " 'For a \"no budget\" movie this thing rocks. I don\\'t know if America\\'s gonna like it, but we were laughing all the way through. Some really Funny Funny stuff. Really non-Hollywood.<br /><br />The Actors and Music rocked. The cars and gags',\n",
       " 'RKO had a reputation for making folksy, homespun pieces of Americana.<br /><br />Anne Shirley (as Dawn O\\'Day) had been in films since she was a toddler. By 1933 she was in limbo - having played Ann Dvorak as a child in \"Three',\n",
       " \"I just can't understand the negative comments about this film. Yes it is a typical boy-meets-girl romance but it is done with such flair and polish that the time just flies by. Henstridge (talk about winning the gene-pool lottery!) is as magnetic and alluring as\",\n",
       " 'Add pure humor + quick and unique sentences + sex + unfaith sex! + love + lies + dark deadly thoughts + secret plans + fun + black humor + sex!.. again! + black dresses! (needed for the unlimited funerals!) = Eglimata!!! Or in English, Crimes!!']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[hooked_model.tokenizer.decode(i) for i in input]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode & Save Representations From Final Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run input through model and save the activations\n",
    "res, cache = hooked_model.run_with_cache(input)\n",
    "pre_cls_END_hidden_state = cache[\"ln_final.hook_normalized\"][:,-1,:].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5883, 0.4117],\n",
       "        [0.4859, 0.5141],\n",
       "        [0.5344, 0.4656],\n",
       "        [0.4422, 0.5578],\n",
       "        [0.4747, 0.5253],\n",
       "        [0.4171, 0.5829],\n",
       "        [0.4068, 0.5932],\n",
       "        [0.4570, 0.5430],\n",
       "        [0.7247, 0.2753],\n",
       "        [0.2514, 0.7486],\n",
       "        [0.5676, 0.4324],\n",
       "        [0.4756, 0.5244],\n",
       "        [0.4087, 0.5913],\n",
       "        [0.4537, 0.5463],\n",
       "        [0.6197, 0.3803],\n",
       "        [0.4384, 0.5616],\n",
       "        [0.3924, 0.6076],\n",
       "        [0.6890, 0.3110],\n",
       "        [0.5911, 0.4089],\n",
       "        [0.6026, 0.3974],\n",
       "        [0.4674, 0.5326],\n",
       "        [0.6413, 0.3587],\n",
       "        [0.6988, 0.3012],\n",
       "        [0.5720, 0.4280],\n",
       "        [0.7467, 0.2533],\n",
       "        [0.2503, 0.7497],\n",
       "        [0.4740, 0.5260],\n",
       "        [0.4775, 0.5225],\n",
       "        [0.4226, 0.5774],\n",
       "        [0.6181, 0.3819],\n",
       "        [0.6271, 0.3729],\n",
       "        [0.6338, 0.3662],\n",
       "        [0.3725, 0.6275],\n",
       "        [0.5419, 0.4581],\n",
       "        [0.8269, 0.1731],\n",
       "        [0.4132, 0.5868],\n",
       "        [0.5973, 0.4027],\n",
       "        [0.4278, 0.5722],\n",
       "        [0.4980, 0.5020],\n",
       "        [0.6866, 0.3134],\n",
       "        [0.5396, 0.4604],\n",
       "        [0.5537, 0.4463],\n",
       "        [0.4084, 0.5916],\n",
       "        [0.4882, 0.5118],\n",
       "        [0.4197, 0.5803],\n",
       "        [0.5725, 0.4275],\n",
       "        [0.5017, 0.4983],\n",
       "        [0.3713, 0.6287],\n",
       "        [0.4119, 0.5881],\n",
       "        [0.5852, 0.4148],\n",
       "        [0.4285, 0.5715],\n",
       "        [0.6033, 0.3967],\n",
       "        [0.4387, 0.5613],\n",
       "        [0.7656, 0.2344],\n",
       "        [0.4537, 0.5463],\n",
       "        [0.7127, 0.2873],\n",
       "        [0.3858, 0.6142],\n",
       "        [0.2818, 0.7182],\n",
       "        [0.6328, 0.3672],\n",
       "        [0.5644, 0.4356],\n",
       "        [0.4483, 0.5517],\n",
       "        [0.4945, 0.5055],\n",
       "        [0.5998, 0.4002],\n",
       "        [0.6294, 0.3706],\n",
       "        [0.5001, 0.4999],\n",
       "        [0.6319, 0.3681],\n",
       "        [0.5796, 0.4204],\n",
       "        [0.6344, 0.3656],\n",
       "        [0.3978, 0.6022],\n",
       "        [0.3919, 0.6081],\n",
       "        [0.4185, 0.5815],\n",
       "        [0.5937, 0.4063],\n",
       "        [0.3398, 0.6602],\n",
       "        [0.6259, 0.3741],\n",
       "        [0.5690, 0.4310],\n",
       "        [0.4366, 0.5634],\n",
       "        [0.4588, 0.5412],\n",
       "        [0.3246, 0.6754],\n",
       "        [0.4356, 0.5644],\n",
       "        [0.3413, 0.6587],\n",
       "        [0.3311, 0.6689],\n",
       "        [0.2091, 0.7909],\n",
       "        [0.3522, 0.6478],\n",
       "        [0.3800, 0.6200],\n",
       "        [0.6995, 0.3005],\n",
       "        [0.2301, 0.7699],\n",
       "        [0.5716, 0.4284],\n",
       "        [0.1642, 0.8358],\n",
       "        [0.3950, 0.6050],\n",
       "        [0.6020, 0.3980],\n",
       "        [0.4255, 0.5745],\n",
       "        [0.5239, 0.4761],\n",
       "        [0.2996, 0.7004],\n",
       "        [0.3818, 0.6182],\n",
       "        [0.4929, 0.5071],\n",
       "        [0.2996, 0.7004],\n",
       "        [0.4048, 0.5952],\n",
       "        [0.5096, 0.4904],\n",
       "        [0.6863, 0.3137],\n",
       "        [0.6144, 0.3856],\n",
       "        [0.4877, 0.5123],\n",
       "        [0.3509, 0.6491],\n",
       "        [0.3017, 0.6983],\n",
       "        [0.6683, 0.3317],\n",
       "        [0.6058, 0.3942],\n",
       "        [0.4128, 0.5872],\n",
       "        [0.8021, 0.1979],\n",
       "        [0.4450, 0.5550],\n",
       "        [0.5495, 0.4505],\n",
       "        [0.6140, 0.3860],\n",
       "        [0.5639, 0.4361],\n",
       "        [0.3534, 0.6466],\n",
       "        [0.5326, 0.4674],\n",
       "        [0.5113, 0.4887],\n",
       "        [0.6848, 0.3152],\n",
       "        [0.3910, 0.6090],\n",
       "        [0.3349, 0.6651],\n",
       "        [0.4104, 0.5896],\n",
       "        [0.4739, 0.5261],\n",
       "        [0.3651, 0.6349],\n",
       "        [0.4145, 0.5855],\n",
       "        [0.3318, 0.6682],\n",
       "        [0.6130, 0.3870],\n",
       "        [0.5946, 0.4054],\n",
       "        [0.6088, 0.3912],\n",
       "        [0.2567, 0.7433],\n",
       "        [0.4133, 0.5867],\n",
       "        [0.6218, 0.3782]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the classification head from the fine-tuned model to predict the sentiment\n",
    "cls_model.score(pre_cls_END_hidden_state).softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_cls_END_hidden_state.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run R-LACE on Representations\n",
    "The next step is to perform R-LACE on the hidden states of the classification tokens in order to generate a projection matrix that will remove the gender representation directions. Before doing so, the experimenters run PCA on the hidden dimensions and reduce them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import torch\n",
    "from rlace import solve_adv_game\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "#ranks = [4]\n",
    "ranks = [1,4,8,16,32,50,64,100]\n",
    "rlace_projs = defaultdict(dict)\n",
    "inlp_projs = defaultdict(dict)\n",
    "finetune_mode = \"no-adv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Encoded Classification Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pre_cls_END_hidden_state\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "        0, 1, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-LACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizers and result dictionaries\n",
    "Ps_rlace, accs_rlace = {}, {}\n",
    "\n",
    "optimizer_class = torch.optim.SGD\n",
    "optimizer_params_P = {\"lr\": 0.005, \"weight_decay\": 1e-4, \"momentum\": 0.0}\n",
    "optimizer_params_predictor = {\"lr\": 0.005, \"weight_decay\": 1e-5, \"momentum\": 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59000/60000. Acc post-projection: 55.469%; best so-far: 46.094%; Maj: 0.781%; Gap: 45.312%; best loss: 19.4298; current loss: 16.0507: 100%|##########| 60000/60000 [17:06<00:00, 58.43it/s]\n",
      "59000/60000. Acc post-projection: 53.906%; best so-far: 46.875%; Maj: 0.781%; Gap: 46.094%; best loss: 19.1482; current loss: 16.6139: 100%|##########| 60000/60000 [16:51<00:00, 59.31it/s]\n",
      "59000/60000. Acc post-projection: 53.906%; best so-far: 46.094%; Maj: 0.781%; Gap: 45.312%; best loss: 19.4298; current loss: 16.6139: 100%|##########| 60000/60000 [16:34<00:00, 60.32it/s]\n",
      "59000/60000. Acc post-projection: 53.125%; best so-far: 46.094%; Maj: 0.781%; Gap: 45.312%; best loss: 19.4298; current loss: 16.8955: 100%|##########| 60000/60000 [17:02<00:00, 58.70it/s]\n",
      "59000/60000. Acc post-projection: 53.906%; best so-far: 46.094%; Maj: 0.781%; Gap: 45.312%; best loss: 19.4298; current loss: 16.6139: 100%|##########| 60000/60000 [16:32<00:00, 60.45it/s]\n",
      "59000/60000. Acc post-projection: 53.125%; best so-far: 46.094%; Maj: 0.781%; Gap: 45.312%; best loss: 19.4298; current loss: 16.8955: 100%|##########| 60000/60000 [17:01<00:00, 58.76it/s]\n",
      "59000/60000. Acc post-projection: 53.125%; best so-far: 46.094%; Maj: 0.781%; Gap: 45.312%; best loss: 19.4298; current loss: 16.8955: 100%|##########| 60000/60000 [17:01<00:00, 58.74it/s]\n",
      "59000/60000. Acc post-projection: 46.875%; best so-far: 46.094%; Maj: 0.781%; Gap: 45.312%; best loss: 19.4298; current loss: 19.1482: 100%|##########| 60000/60000 [17:44<00:00, 56.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run RLACE for each rank\n",
    "data_type = \"train\"\n",
    "\n",
    "for rank in ranks:\n",
    "\n",
    "    output = solve_adv_game(X, y, X, y, rank=rank, device=device, out_iters=60000,\n",
    "                                optimizer_class=optimizer_class, optimizer_params_P=optimizer_params_P,\n",
    "                                optimizer_params_predictor=optimizer_params_predictor, epsilon=0.002,\n",
    "                                batch_size=256)\n",
    "\n",
    "    P = output[\"P\"]\n",
    "    Ps_rlace[rank] = P\n",
    "    accs_rlace[rank] = output[\"score\"]\n",
    "\n",
    "    # Save resulting projection matrices\n",
    "    with open(\"interim/{}/Ps_rlace.pickle\".format(data_type), \"wb\") as f:\n",
    "        pickle.dump((Ps_rlace, accs_rlace), f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply R-LACE Projection to Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, LinearRegression, Lasso, Ridge\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sn\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from sklearn.manifold import TSNE\n",
    "import tqdm\n",
    "import copy\n",
    "from sklearn.svm import LinearSVC \n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import torch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster\n",
    "from sklearn import neural_network\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import warnings\n",
    "import argparse\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from torchtyping import TensorType as TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f9f4a07b730>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample with RLACE Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_projections(proj_type, data_split):\n",
    "    \"\"\"Load the projection matrices for the given projection type and finetune mode\n",
    "\n",
    "    Args:\n",
    "        proj_type (str): projection type (e.g. \"inlp\", \"rlace\")\n",
    "        data_split (str): dataset on which projections were trained (e.g. \"train\", \"valid\")\n",
    "    \"\"\"\n",
    "    with open(\"interim/{}/Ps_{}.pickle\".format(data_split, proj_type), \"rb\") as f:\n",
    "        rank2P = pickle.load(f)\n",
    "        return rank2P\n",
    "    \n",
    "Ps_rlace = load_projections(\"rlace\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def patch_rlace_pos(\n",
    "    orig_resid_vector: TT[\"batch\", \"pos\", \"d_model\"],\n",
    "    hook,\n",
    "    pos=-1, \n",
    "    projection=None,\n",
    "):\n",
    "    orig_resid_vector[:, pos, :] = orig_resid_vector[:, pos, :] @ projection\n",
    "    return orig_resid_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = torch.FloatTensor(Ps_rlace[0][100]).to(device)\n",
    "projection_patching_hook = partial(patch_rlace_pos, pos=-1, projection=projection)\n",
    "logits = hooked_model.run_with_hooks(\n",
    "    input, fwd_hooks=[(\"ln_final.hook_normalized\", projection_patching_hook)]\n",
    ")[:,-1,:]\n",
    "#pre_cls_END_hidden_state = cache[\"ln_final.hook_normalized\"][:,-1,:].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Union, List, Tuple, Dict, Optional, NamedTuple, overload\n",
    "from typing_extensions import Literal\n",
    "from jaxtyping import Float, Int\n",
    "from transformer_lens.past_key_value_caching import (\n",
    "    HookedTransformerKeyValueCache,\n",
    ")\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_with_hooks(\n",
    "    input: Union[str, Float[torch.Tensor, \"batch pos\"]] = \"\",\n",
    "    max_new_tokens: int = 10,\n",
    "    stop_at_eos: bool = True,\n",
    "    eos_token_id: Optional[int] = None,\n",
    "    do_sample: bool = False,\n",
    "    top_k: Optional[int] = None,\n",
    "    top_p: Optional[float] = None,\n",
    "    temperature: float = 1.0,\n",
    "    freq_penalty: float = 0.0,\n",
    "    num_return_sequences: int = 1,\n",
    "    use_past_kv_cache: bool = True,\n",
    "    prepend_bos=True,\n",
    "    return_type: Optional[str] = \"input\",\n",
    "    fwd_hooks: Optional[List[Tuple[str, Callable]]] = None,\n",
    ") -> Float[torch.Tensor, \"batch pos_plus_new_tokens\"]:\n",
    "    \"\"\"\n",
    "    Sample tokens from the model until the model outputs eos_token or max_new_tokens is reached.\n",
    "\n",
    "    To avoid fiddling with ragged tensors, if we input a batch of text and some sequences finish (by producing an EOT token), we keep running the model on the entire batch, but throw away the output for a finished sequence and just keep adding EOTs to pad.\n",
    "\n",
    "    This supports entering a single string, but not a list of strings - if the strings don't tokenize to exactly the same length, this gets messy. If that functionality is needed, convert them to a batch of tokens and input that instead.\n",
    "\n",
    "    Args:\n",
    "        input (int): Either a batch of tokens ([batch, pos]) or a text string (this will be converted to a batch of tokens with batch size 1)\n",
    "        max_new_tokens (int): Maximum number of tokens to generate\n",
    "        stop_at_eos (bool): If True, stop generating tokens when the model outputs eos_token\n",
    "        eos_token_id (int, *optional*): The token ID to use for end of sentence. If None, use the tokenizer's eos_token_id - required if using stop_at_eos\n",
    "        do_sample (bool): If True, sample from the model's output distribution. Otherwise, use greedy search (take the max logit each time).\n",
    "        top_k (int): Number of tokens to sample from. If None, sample from all tokens\n",
    "        top_p (float): Probability mass to sample from. If 1.0, sample from all tokens. If <1.0, we take the top tokens with cumulative probability >= top_p\n",
    "        temperature (float): Temperature for sampling. Higher values will make the model more random (limit of temp -> 0 is just taking the top token, limit of temp -> inf is sampling from a uniform distribution)\n",
    "        freq_penalty (float): Frequency penalty for sampling - how much to penalise previous tokens. Higher values will make the model more random\n",
    "        use_past_kv_cache (bool): If True, create and use cache to speed up generation\n",
    "        prepend_bos (bool): If True, prepend the model's bos_token_id to the input, if it's a string. Irrelevant if input is a tensor.\n",
    "        return_type (str, *optional*): The type of the output to return - either a string (str), a tensor of tokens (tensor) or whatever the format of the input was (input).\n",
    "    Returns:\n",
    "        outputs (torch.Tensor): [batch, pos + max_new_tokens], generated sequence of new tokens - by default returns same type as input\n",
    "    \"\"\"\n",
    "    if type(input) == str:\n",
    "        # If text, convert to tokens (batch_size=1)\n",
    "        assert (\n",
    "            hooked_model.tokenizer is not None\n",
    "        ), \"Must provide a tokenizer if passing a string to the model\"\n",
    "        tokens = hooked_model.to_tokens(input, prepend_bos=prepend_bos)\n",
    "    else:\n",
    "        tokens = input\n",
    "\n",
    "    if return_type == \"input\":\n",
    "        if type(input) == str:\n",
    "            return_type = \"str\"\n",
    "        else:\n",
    "            return_type = \"tensor\"\n",
    "\n",
    "    assert isinstance(tokens, torch.Tensor)\n",
    "    batch_size, ctx_length = tokens.shape\n",
    "    tokens = tokens.to(hooked_model.cfg.device)\n",
    "    if use_past_kv_cache:\n",
    "        past_kv_cache = HookedTransformerKeyValueCache.init_cache(\n",
    "            hooked_model.cfg, hooked_model.cfg.device, batch_size\n",
    "        )\n",
    "    else:\n",
    "        past_kv_cache = None\n",
    "\n",
    "    if stop_at_eos and eos_token_id is None:\n",
    "        assert (\n",
    "            hooked_model.tokenizer is not None and hooked_model.tokenizer.eos_token_id is not None\n",
    "        ), \"Must pass a eos_token_id if stop_at_eos is True and tokenizer is None or has no eos_token_id\"\n",
    "\n",
    "        eos_token_id = hooked_model.tokenizer.eos_token_id\n",
    "\n",
    "    # An array to track which sequences in the batch have finished.\n",
    "    finished_sequences = torch.zeros(\n",
    "        batch_size, dtype=torch.bool, device=hooked_model.cfg.device\n",
    "    )\n",
    "\n",
    "    # Currently nothing in HookedTransformer changes with eval, but this is here in case that changes in the future\n",
    "    hooked_model.eval()\n",
    "    for index in tqdm.tqdm(range(max_new_tokens)):\n",
    "        # While generating, we keep generating logits, throw away all but the final logits, and then use those logits to sample from the distribution\n",
    "        # We keep adding the sampled tokens to the end of tokens.\n",
    "        if use_past_kv_cache:\n",
    "            # We just take the final tokens, as a [batch, 1] tensor\n",
    "            if index > 0:\n",
    "                # logits = hooked_model.forward(\n",
    "                #     tokens[:, -1:],\n",
    "                #     return_type=\"logits\",\n",
    "                    \n",
    "                # )\n",
    "                logits = hooked_model.run_with_hooks(\n",
    "                    tokens[:, -1:],\n",
    "                    return_type=\"logits\", \n",
    "                    fwd_hooks=[(\"ln_final.hook_normalized\", projection_patching_hook)],\n",
    "                    past_kv_cache=past_kv_cache,\n",
    "                )\n",
    "            else:\n",
    "                logits = hooked_model.run_with_hooks(\n",
    "                    tokens,\n",
    "                    return_type=\"logits\", \n",
    "                    fwd_hooks=[(\"ln_final.hook_normalized\", projection_patching_hook)],\n",
    "                    past_kv_cache=past_kv_cache,\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # We input the entire sequence, as a [batch, pos] tensor, since we aren't using the cache\n",
    "            logits = hooked_model.run_with_hooks(\n",
    "                    tokens,\n",
    "                    return_type=\"logits\", \n",
    "                    fwd_hooks=[(\"ln_final.hook_normalized\", projection_patching_hook)],\n",
    "                )\n",
    "        final_logits = logits[:, -1, :]\n",
    "\n",
    "        sampled_tokens = utils.sample_logits(\n",
    "            final_logits,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            freq_penalty=freq_penalty,\n",
    "            tokens=tokens,\n",
    "        )\n",
    "\n",
    "        if stop_at_eos:\n",
    "            # For all unfinished sequences, add on the next token. If a sequence finished, we throw away the generated token and instead add an EOS token to pad.\n",
    "            sampled_tokens[finished_sequences] = eos_token_id\n",
    "            finished_sequences.logical_or_(sampled_tokens == eos_token_id)\n",
    "\n",
    "        tokens = torch.cat([tokens, sampled_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        if stop_at_eos and finished_sequences.all():\n",
    "            break\n",
    "\n",
    "    if return_type == \"str\":\n",
    "        if prepend_bos:\n",
    "            # If we prepended a BOS token, remove it when returning output.\n",
    "            return hooked_model.tokenizer.decode(tokens[0, 1:])\n",
    "        else:\n",
    "            return hooked_model.tokenizer.decode(tokens[0])\n",
    "\n",
    "    else:\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 89.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The movie Avatar was great. The characters were there again and they did everything they did for the movie. Boston, New York, they did it all together. Boston, I wanted them to do it again and they did it. Boston, I want them to do it again and they did it. Boston, I want them to do as well and they did it again. Boston Kidd tough, Kidd tough, I wanted them guys to do it again and they did it. Boston Kidd tough Kidd, I want them guys else'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With RLACE\n",
    "generate_with_hooks(\n",
    "    \"The movie Avatar was great\", \n",
    "    max_new_tokens=100, \n",
    "    stop_at_eos=True, \n",
    "    temperature=0.5,\n",
    "    use_past_kv_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb224b149cd48ecad99067e873fd4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The movie Avatar was great. I was really impressed with the way the characters were portrayed. The story was well written, the characters were believable, and the dialogue was funny. The story was also well received, and the story was so well written that I really liked the way the film was narrated. The story was also so well written that I really enjoyed the way the characters were portrayed. The movie was great. I was really impressed with the way the characters were portrayed. The story was well written, the characters were believable'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No RLACE\n",
    "hooked_model.generate(\"The movie Avatar was great\", max_new_tokens=100, stop_at_eos=True, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_name = \"ln_final.hook_normalized\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_name = \"ln_final.hook_normalized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_patching_hook = partial(patch_rlace_pos, pos=-1, projection=projection)\n",
    "hooked_model.mod_dict[hook_name].add_hook(projection_patching_hook, dir=\"fwd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_logits, inf_cache = hooked_model.run_with_cache(input, return_type=\"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, sequence_length = input.shape[:2]\n",
    "sequence_lengths = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre_cls_hidden_states = (inf_cache[hook_name] / inf_cache[\"ln_final.hook_scale\"]).cpu()\n",
    "pre_cls_hidden_states = (inf_cache[hook_name]).cpu()\n",
    "rlace_logits = cls_model.score(pre_cls_hidden_states)\n",
    "pooled_logits = rlace_logits[range(batch_size), sequence_lengths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlace_classification = pooled_logits.softmax(dim=-1).argmax(dim=-1)\n",
    "rlace_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_classification = cls_model(input)['logits'].softmax(dim=-1).argmax(dim=-1)\n",
    "original_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7188), tensor(0.5469))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlace_accuracy = (rlace_classification == labels).float().mean()\n",
    "original_accuracy = (original_classification == labels).float().mean()\n",
    "original_accuracy, rlace_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "0dda8425f5b2686a7d4649dd29b2ea64fce78f14efba812f7700f06d7544e589"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
